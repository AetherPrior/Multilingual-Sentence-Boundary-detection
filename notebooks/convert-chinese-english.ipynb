{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Creator -- English and Chinese  \n",
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Open IWSLT Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path_zh = \"../dataset/zh-en/\"\n",
    "\n",
    "with open(data_path_zh + 'train_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text_zh = f.readlines()\n",
    "with open(data_path_zh + 'dev_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text_zh = f.readlines()\n",
    "with open(data_path_zh + 'test_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text_zh = f.readlines()\n",
    "\n",
    "data_path_en = \"../dataset/en-fr/\"\n",
    "\n",
    "with open(data_path_en + 'train_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text_en = f.readlines()\n",
    "with open(data_path_en + 'dev_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text_en = f.readlines()\n",
    "with open(data_path_en + 'test_texts_2012.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text_en = f.readlines()\n",
    "\n",
    "'''\n",
    "train_text = random.shuffle(train_text_en+train_text_zh)\n",
    "valid_text = random.shuffle(valid_text_en+valid_text_zh)\n",
    "test_text  = random.shuffle(test_text_en+test_text_zh)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntrain_text = random.shuffle(train_text_en+train_text_zh)\\nvalid_text = random.shuffle(valid_text_en+valid_text_zh)\\ntest_text  = random.shuffle(test_text_en+test_text_zh)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "datasets = train_text_en, valid_text_en, test_text_en\n",
    "datasets_zh = train_text_zh, valid_text_zh, test_text_zh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def clean_text_zh(text):\n",
    "    text = text.replace('！', '。')\n",
    "    text = text.replace('：', '，')\n",
    "    text = text.replace('——', '，')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s—\\s', ' ， ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '。')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace('、', '，')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('……', '')\n",
    "    text = text.replace('。”', '')\n",
    "    text = text.replace('”', '，')\n",
    "    text = text.replace('“','，')\n",
    "    text = text.replace(',','，')\n",
    "    \n",
    "\n",
    "    text = re.sub(r'——\\s?——', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r'，\\s?，', '，', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r'，\\s?。', '。', text) # , . -> ,\n",
    "    text = re.sub(r'？\\s?。', '？', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+？', '？', text)\n",
    "    text = re.sub(r'\\s+，', '，', text)\n",
    "    text = re.sub(r'。[\\s+。]+', '。 ', text)\n",
    "    text = re.sub(r'\\s+。', '。 ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def clean_text_en(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    r = re.compile(reg, re.DOTALL)\n",
    "    text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r',\\s?\\.', '.', text) # , . -> ,\n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9]),(?=[a-zA-Z0-9])',', ',text) # say,you -> say, you\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "datasets_en = train_text_en, valid_text_en, test_text_en\n",
    "datasets_zh = train_text_zh, valid_text_zh, test_text_zh\n",
    "\n",
    "datasets_zh = [[clean_text_zh(text) for text in ds] for ds in datasets_zh]\n",
    "datasets_en = [[clean_text_en(text) for text in ds] for ds in datasets_en]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "target_token2id_en = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id_zh = {t: tokenizer.encode(t)[-2] for t in \"。?,\"}\n",
    "target_ids_en = list(target_token2id_en.values())\n",
    "target_ids_zh = list(target_token2id_zh.values())\n",
    "target_ids_en, target_ids_zh"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([5, 705, 4], [30, 705, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "id2target_zh = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids_zh):\n",
    "    id2target_zh[ti] = i+1\n",
    "target2id_zh = {value: key for key, value in id2target_zh.items()}\n",
    "print(id2target_zh, target2id_zh)\n",
    "def create_target_zh(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = tokenizer.tokenize(text)[1:] ## ignore the first space\n",
    "\n",
    "    words2 = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "            if i < len(words) -1 and words[i+1] in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "                words2.append(words[i])\n",
    "            else:\n",
    "                words2.extend([words[i],' '])\n",
    "        else:\n",
    "            if words[i] == \"▁\":\n",
    "                if i > 0 and words[i-1] not in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "                    words2.append(\" \")\n",
    "            else:\n",
    "                words2.append(words[i])\n",
    "            \n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        target = 0\n",
    "        target_appended = False\n",
    "        for target_token, target_id in target_token2id_zh.items():\n",
    "            if word == target_token:\n",
    "                #word = word.rstrip(target_token)\n",
    "                encoded_words.append(target_token2id_zh[word])\n",
    "                targets.append(id2target_zh[target_id])\n",
    "                target_appended = True\n",
    "        if not target_appended:\n",
    "            if word == ' ':\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "            else:    \n",
    "                encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "                if len(encoded_word) == 2:\n",
    "                    encoded_word = encoded_word[1:]\n",
    "\n",
    "                for w in encoded_word:\n",
    "                    encoded_words.append(w)\n",
    "\n",
    "                if len(encoded_word)>1:\n",
    "                    for _ in range(len(encoded_word)-1):\n",
    "                        if encoded_word[_] == 6:\n",
    "                            targets.append(0)\n",
    "                        else:\n",
    "                            targets.append(-1)\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(0)    \n",
    "\n",
    "#             print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "            assert(len(encoded_word)>0)\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3} {0: 0, -1: -1, 1: 30, 2: 705, 3: 4}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "id2target_en = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids_en):\n",
    "    id2target_en[ti] = i+1\n",
    "target2id_en = {value: key for key, value in id2target_en.items()}\n",
    "\n",
    "def create_target_en(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id_en.items():\n",
    "            if word.endswith(target_token):\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target_en[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "        targets.append(0)\n",
    "        \n",
    "        if target != 0:\n",
    "            encoded_words.append(target2id_en[target])\n",
    "        else:\n",
    "            encoded_words.append(6)\n",
    "        targets.append(target)\n",
    "        \n",
    "        \n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        assert(len(encoded_word)>0)\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "# sentence endings split\n",
    "encoded_texts_zh, targets_zh = [], []\n",
    "\n",
    "for ds in datasets_zh:\n",
    "    trgts = []\n",
    "    '''\n",
    "    for ts in ds:\n",
    "        prev = 0\n",
    "        init = 0\n",
    "        #print(\"Length of sequence: \",len(ts))\n",
    "        for i in range(len(ts)):\n",
    "            if  ts[i] in [\"。\",\".\",\"？\",\"?\"]:\n",
    "                if i > init+511:\n",
    "                    if prev == 0:\n",
    "                        #print(\"truncating first sentence\")\n",
    "                        trgts.append(ts[0:512])\n",
    "                        prev = 511\n",
    "                        init = 511\n",
    "                    else:\n",
    "                        if prev == init: \n",
    "                            prev = i\n",
    "                        #print(\"appending from \",init,\" to \",prev)\n",
    "                        if prev - init > 511:\n",
    "                            #print(\"CHUNKing sentence\")\n",
    "                            ls = ts[init+1:prev+1]\n",
    "                            trgts.extend([ls[i:i+512] for i in range(0,prev-init,512)])\n",
    "                        else:\n",
    "                            trgts.append(ts[init+1:prev+1])\n",
    "                        init = prev\n",
    "                        prev = init\n",
    "                else:\n",
    "                    prev = i\n",
    "        \n",
    "        if prev < len(ts)-1:\n",
    "            #print(\"appending last sentence from \",prev,\" to \",len(ts)-1)\n",
    "            #if(len(ts)-1 - prev > 511):\n",
    "            #    #print(\"chunking last sentence\")\n",
    "            trgts.extend([ts[i:i+512] for i in range(prev,len(ts)-1,512)])\n",
    "            #trgts.append(ts[prev:len(ts)])\n",
    "    '''\n",
    "    x = list(zip(*(create_target_zh(trgt) for trgt in tqdm(ds)))) # use \"trgts\" instead of \"ds\" if you want 512, the warning can be ignored\n",
    "    encoded_texts_zh.append(x[0])\n",
    "    targets_zh.append(x[1])\n",
    "    \n",
    "encoded_texts_en, targets_en = [], []\n",
    "\n",
    "for ds in datasets_en:\n",
    "    x = list(zip(*(create_target_en(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts_en.append(x[0])\n",
    "    targets_en.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1017 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288b238f40094ac28f4530bcd239706d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3275 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce502c1babd14ba6838d7945cdbce406"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9096a214d2a40f3a6cf614f9d299c59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b44ad67e0c0f4e659e80c21aee0c93b2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b39f526cfffe403e819b5078c68c01ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e86ad94a8d7d49e88b35a85bc9e52afc"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "print(id2target_zh)\n",
    "s = \"谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\"\n",
    "print(s)\n",
    "s = clean_text_zh(s)\n",
    "print(s)\n",
    "data, tgts = create_target_zh(s)\n",
    "#print(data)\n",
    "#print(targets)\n",
    "print([(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[2:-1], tgts[2:-1])])\n",
    "\n",
    "print(id2target_en)\n",
    "# s = \"Tyranosaurus: kill me? Not enough, rumplestilskin -- said the co-pilot -- ...\"\n",
    "s = \"it  can  be  a  very  complicated  thing, the  ocean. and  it  can  be  a  very  complicated  thing, what  human  health  is.\"\n",
    "print(s)\n",
    "s = clean_text_en(s)\n",
    "print(s)\n",
    "data, tgts = create_target_en(s)\n",
    "print(data)\n",
    "print(tgts)\n",
    "print([(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[1:-1], tgts[1:-1])])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3}\n",
      "谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\n",
      "谁能猜一猜，你大脑里神经元的总长有多少？，西班牙厨师被控告， 非常坚硬的土地。西班牙厨师被控告\n",
      "[('▁', 0), ('能', 0), ('▁', 0), ('猜', 0), ('▁', 0), ('▁一', 0), ('▁', 0), ('猜', 0), (',', 3), ('▁你', 0), ('▁', 0), ('大脑', 0), ('▁', 0), ('里', 0), ('▁', 0), ('神经', 0), ('▁', 0), ('元的', 0), ('▁', 0), ('总', 0), ('▁', 0), ('长', 0), ('▁', 0), ('有多少', 0), ('▁?', 2), (',', 3), ('西班牙', 0), ('▁', 0), ('厨', 0), ('▁', 0), ('师', 0), ('▁', 0), ('被', 0), ('▁', 0), ('控', 0), ('▁', 0), ('告', 0), (',', 3), ('非常', 0), ('▁', 0), ('坚', 0), ('▁', 0), ('硬', 0), ('▁', 0), ('的', 0), ('▁', 0), ('土地', 0), ('。', 1), ('西班牙', 0), ('▁', 0), ('厨', 0), ('▁', 0), ('师', 0), ('▁', 0), ('被', 0), ('▁', 0), ('控', 0), ('▁', 0), ('告', 0), ('▁', 0)]\n",
      "{0: 0, -1: -1, 5: 1, 705: 2, 4: 3}\n",
      "it  can  be  a  very  complicated  thing, the  ocean. and  it  can  be  a  very  complicated  thing, what  human  health  is.\n",
      "it can be a very complicated thing, the ocean. and it can be a very complicated thing, what human health is.\n",
      "[0, 442, 6, 831, 6, 186, 6, 10, 6, 4552, 6, 96704, 297, 6, 13580, 4, 70, 6, 77904, 5, 136, 6, 442, 6, 831, 6, 186, 6, 10, 6, 4552, 6, 96704, 297, 6, 13580, 4, 2367, 6, 14135, 6, 16227, 6, 83, 5, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, -1]\n",
      "[('▁it', 0), ('▁', 0), ('▁can', 0), ('▁', 0), ('▁be', 0), ('▁', 0), ('▁a', 0), ('▁', 0), ('▁very', 0), ('▁', 0), ('▁complicat', -1), ('ed', 0), ('▁', 0), ('▁thing', 0), (',', 3), ('▁the', 0), ('▁', 0), ('▁ocean', 0), ('.', 1), ('▁and', 0), ('▁', 0), ('▁it', 0), ('▁', 0), ('▁can', 0), ('▁', 0), ('▁be', 0), ('▁', 0), ('▁a', 0), ('▁', 0), ('▁very', 0), ('▁', 0), ('▁complicat', -1), ('ed', 0), ('▁', 0), ('▁thing', 0), (',', 3), ('▁what', 0), ('▁', 0), ('▁human', 0), ('▁', 0), ('▁health', 0), ('▁', 0), ('▁is', 0), ('.', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "data_path_dual = \"../dataset/en-zh-dual/\"\n",
    "os.makedirs(data_path_dual + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = map(sum, list(zip(*(return_counts(encoded_texts_zh,targets_zh),return_counts(encoded_texts_en,targets_en)))))\n",
    "space_count, p_count, q_count, comma_count"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4101189, 227507, 21315, 355172)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "encoded_texts = [encoded_texts_en[i]+encoded_texts_zh[i] for i in range(len(encoded_texts_en))]\n",
    "targets = [targets_en[i]+targets_zh[i] for i in range(len(encoded_texts_en))]\n",
    "enc = [list(zip(encoded_texts[i],targets[i])) for i in range(len(encoded_texts))]\n",
    "temp = [random.sample(enc[i], len(enc[i])) for i in range(len(encoded_texts))]\n",
    "encoded_texts = []\n",
    "targets = []\n",
    "for i in temp:\n",
    "    a,b = list(zip(*i))\n",
    "    encoded_texts.append(a)\n",
    "    targets.append(b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(0,100):\n",
    "    print(len(encoded_texts[0][i]),len(targets[0][i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path_dual + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "print(list(zip([tokenizer.convert_ids_to_tokens(token) for token in encoded_texts[0][9][0:20]],targets[0][9][0:20])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('<s>', -1), ('有趣的', 0), ('▁', 0), ('是', 0), (',', 3), ('查', 0), ('▁', 0), ('尔斯', 0), ('▁', 0), ('▁-', 0), ('▁', 0), ('达', 0), ('▁', 0), ('尔', 0), ('▁', 0), ('文', 0), ('▁', 0), ('一个', 0), ('▁', 0), ('肤', 0)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('mPunct': venv)"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}