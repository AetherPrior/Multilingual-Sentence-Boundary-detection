{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Creator -- English and Chinese  \n",
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Open IWSLT Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path_zh = \"../dataset/zh-en/\"\n",
    "\n",
    "with open(data_path_zh + 'train_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text_zh = f.readlines()\n",
    "with open(data_path_zh + 'dev_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text_zh = f.readlines()\n",
    "with open(data_path_zh + 'test_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text_zh = f.readlines()\n",
    "\n",
    "data_path_en = \"../dataset/en-fr/\"\n",
    "\n",
    "with open(data_path_en + 'train_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text_en = f.readlines()\n",
    "with open(data_path_en + 'dev_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text_en = f.readlines()\n",
    "with open(data_path_en + 'test_texts_2012.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text_en = f.readlines()\n",
    "\n",
    "'''\n",
    "train_text = random.shuffle(train_text_en+train_text_zh)\n",
    "valid_text = random.shuffle(valid_text_en+valid_text_zh)\n",
    "test_text  = random.shuffle(test_text_en+test_text_zh)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntrain_text = random.shuffle(train_text_en+train_text_zh)\\nvalid_text = random.shuffle(valid_text_en+valid_text_zh)\\ntest_text  = random.shuffle(test_text_en+test_text_zh)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "datasets = train_text_en, valid_text_en, test_text_en\n",
    "datasets_zh = train_text_zh, valid_text_zh, test_text_zh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def clean_text_zh(text):\n",
    "    text = text.replace('！', '。')\n",
    "    text = text.replace('：', '，')\n",
    "    text = text.replace('——', '，')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s—\\s', ' ， ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '。')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace('、', '，')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('……', '')\n",
    "    text = text.replace('。”', '')\n",
    "    text = text.replace('”', '，')\n",
    "    text = text.replace('“','，')\n",
    "    text = text.replace(',','，')\n",
    "    \n",
    "\n",
    "    text = re.sub(r'——\\s?——', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r'，\\s?，', '，', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r'，\\s?。', '。', text) # , . -> ,\n",
    "    text = re.sub(r'？\\s?。', '？', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+？', '？', text)\n",
    "    text = re.sub(r'\\s+，', '，', text)\n",
    "    text = re.sub(r'。[\\s+。]+', '。 ', text)\n",
    "    text = re.sub(r'\\s+。', '。 ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def clean_text_en(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    r = re.compile(reg, re.DOTALL)\n",
    "    text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r',\\s?\\.', '.', text) # , . -> ,\n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9]),(?=[a-zA-Z0-9])',', ',text) # say,you -> say, you\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "datasets_en = train_text_en, valid_text_en, test_text_en\n",
    "datasets_zh = train_text_zh, valid_text_zh, test_text_zh\n",
    "\n",
    "datasets_zh = [[clean_text_zh(text) for text in ds] for ds in datasets_zh]\n",
    "datasets_en = [[clean_text_en(text) for text in ds] for ds in datasets_en]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "target_token2id_en = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id_zh = {t: tokenizer.encode(t)[-2] for t in \"。？，\"}\n",
    "target_ids_en = list(target_token2id_en.values())\n",
    "target_ids_zh = list(target_token2id_zh.values())\n",
    "target_ids_en, target_ids_zh"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([5, 705, 4], [30, 705, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "id2target_zh = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids_zh):\n",
    "    id2target_zh[ti] = i+1\n",
    "target2id_zh = {value: key for key, value in id2target_zh.items()}\n",
    "print(id2target_zh, target2id_zh)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3} {0: 0, -1: -1, 1: 30, 2: 705, 3: 4}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import jieba\n",
    "id2target_en = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids_en):\n",
    "    id2target_en[ti] = i+1\n",
    "target2id_en = {value: key for key, value in id2target_en.items()}\n",
    "\n",
    "def create_target_zh(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = list(jieba.cut(text,HMM=True)) ## ignore the first space\n",
    "    words2 = []\n",
    "    for i in range(len(words)):\n",
    "        encoded_word = tokenizer.encode(words[i])\n",
    "        #print(words[i],encoded_word)\n",
    "        if (len(encoded_word[1:-1]) > 1 and encoded_word[1] != 6) or (len(encoded_word[1:-1]) > 2 and encoded_word[1] == 6):\n",
    "            for word in encoded_word[1:-1]:\n",
    "                if word != 6:\n",
    "                    encoded_words.append(word)\n",
    "                    targets.append(-1)\n",
    "            targets = targets[:-1]   \n",
    "        elif len(encoded_word[1:-1]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            #print(\"Here! \",encoded_word)\n",
    "            s = 2 if encoded_word[1] == 6 else 1\n",
    "            encoded_words.append(encoded_word[s])\n",
    "                \n",
    "            \n",
    "            \n",
    "        if words[i] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "            if i < len(words) -1 and words[i+1] in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                ##words2.append(words[i])\n",
    "                targets.append(0)\n",
    "                pass\n",
    "            else:\n",
    "                targets.append(0)\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "        else:\n",
    "            if words[i] in [\"▁\",\" \"]:\n",
    "                if i > 0 and words[i-1] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                    #encoded_words.append(\" \")\n",
    "                    #targets.append(0)\n",
    "                    pass\n",
    "            else:\n",
    "                #print(\"YES\",words[i])\n",
    "                targets.append(id2target_zh[target_token2id_zh[words[i]]])\n",
    "                # words2.append(words[i])\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1]+ targets + [-1]    \n",
    "    \n",
    "    return encoded_words, targets\n",
    "\n",
    "def create_target_en(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id_en.items():\n",
    "            if word.endswith(target_token):\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target_en[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "        targets.append(0)\n",
    "        \n",
    "        if target != 0:\n",
    "            encoded_words.append(target2id_en[target])\n",
    "        else:\n",
    "            encoded_words.append(6)\n",
    "        targets.append(target)\n",
    "        \n",
    "        \n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        assert(len(encoded_word)>0)\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# sentence endings split\n",
    "encoded_texts_zh, targets_zh = [], []\n",
    "\n",
    "for ds in datasets_zh:\n",
    "    trgts = []\n",
    "    '''\n",
    "    for ts in ds:\n",
    "        prev = 0\n",
    "        init = 0\n",
    "        #print(\"Length of sequence: \",len(ts))\n",
    "        for i in range(len(ts)):\n",
    "            if  ts[i] in [\"。\",\".\",\"？\",\"?\"]:\n",
    "                if i > init+511:\n",
    "                    if prev == 0:\n",
    "                        #print(\"truncating first sentence\")\n",
    "                        trgts.append(ts[0:512])\n",
    "                        prev = 511\n",
    "                        init = 511\n",
    "                    else:\n",
    "                        if prev == init: \n",
    "                            prev = i\n",
    "                        #print(\"appending from \",init,\" to \",prev)\n",
    "                        if prev - init > 511:\n",
    "                            #print(\"CHUNKing sentence\")\n",
    "                            ls = ts[init+1:prev+1]\n",
    "                            trgts.extend([ls[i:i+512] for i in range(0,prev-init,512)])\n",
    "                        else:\n",
    "                            trgts.append(ts[init+1:prev+1])\n",
    "                        init = prev\n",
    "                        prev = init\n",
    "                else:\n",
    "                    prev = i\n",
    "        \n",
    "        if prev < len(ts)-1:\n",
    "            #print(\"appending last sentence from \",prev,\" to \",len(ts)-1)\n",
    "            #if(len(ts)-1 - prev > 511):\n",
    "            #    #print(\"chunking last sentence\")\n",
    "            trgts.extend([ts[i:i+512] for i in range(prev,len(ts)-1,512)])\n",
    "            #trgts.append(ts[prev:len(ts)])\n",
    "    '''\n",
    "    x = list(zip(*(create_target_zh(trgt) for trgt in tqdm(ds)))) # use \"trgts\" instead of \"ds\" if you want 512, the warning can be ignored\n",
    "    encoded_texts_zh.append(x[0])\n",
    "    targets_zh.append(x[1])\n",
    "    \n",
    "encoded_texts_en, targets_en = [], []\n",
    "\n",
    "for ds in datasets_en:\n",
    "    x = list(zip(*(create_target_en(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts_en.append(x[0])\n",
    "    targets_en.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90cc487d0b6b41a7b58de51013553288"
      },
      "text/plain": [
       "  0%|          | 0/1017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b978ea5fa584b17b018741ea14fae42"
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10c23930c33c4072a16452f64c7d9535"
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4285e7ea55dc4ca9aa4db70ec7227fdd"
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb6f0fba16214bb299734d8404a13a32"
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf05e0ac0f8340899b67ef6b63d730f9"
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(id2target_zh)\n",
    "s = \"谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\"\n",
    "print(s)\n",
    "s = clean_text_zh(s)\n",
    "print(s)\n",
    "data, tgts = create_target_zh(s)\n",
    "#print(data)\n",
    "#print(targets)\n",
    "print([(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[2:-1], tgts[2:-1])])\n",
    "\n",
    "print(id2target_en)\n",
    "# s = \"Tyranosaurus: kill me? Not enough, rumplestilskin -- said the co-pilot -- ...\"\n",
    "s = \"it  can  be  a  very  complicated  thing, the  ocean. and  it  can  be  a  very  complicated  thing, what  human  health  is.\"\n",
    "print(s)\n",
    "s = clean_text_en(s)\n",
    "print(s)\n",
    "data, tgts = create_target_en(s)\n",
    "print(data)\n",
    "print(tgts)\n",
    "print([(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[1:-1], tgts[1:-1])])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3}\n",
      "谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\n",
      "谁能猜一猜，你大脑里神经元的总长有多少？，西班牙厨师被控告， 非常坚硬的土地。西班牙厨师被控告\n",
      "[('▁', 0), ('能', 0), ('▁', 0), ('猜', 0), ('▁', 0), ('▁一', 0), ('▁', 0), ('猜', 0), (',', 3), ('▁你', 0), ('▁', 0), ('大脑', 0), ('▁', 0), ('里', 0), ('▁', 0), ('神经', -1), ('元', 0), ('▁', 0), ('的', 0), ('▁', 0), ('总', -1), ('长', 0), ('▁', 0), ('有', 0), ('▁', 0), ('多少', 0), ('▁?', 2), (',', 3), ('西班牙', 0), ('▁', 0), ('厨', -1), ('师', 0), ('▁', 0), ('被', 0), ('▁', 0), ('控', -1), ('告', 0), (',', 3), ('非常', 0), ('▁', 0), ('坚', -1), ('硬', 0), ('▁', 0), ('的', 0), ('▁', 0), ('土地', 0), ('。', 1), ('西班牙', 0), ('▁', 0), ('厨', -1), ('师', 0), ('▁', 0), ('被', 0), ('▁', 0), ('控', -1), ('告', 0), ('▁', 0)]\n",
      "{0: 0, -1: -1, 5: 1, 705: 2, 4: 3}\n",
      "it  can  be  a  very  complicated  thing, the  ocean. and  it  can  be  a  very  complicated  thing, what  human  health  is.\n",
      "it can be a very complicated thing, the ocean. and it can be a very complicated thing, what human health is.\n",
      "[0, 442, 6, 831, 6, 186, 6, 10, 6, 4552, 6, 96704, 297, 6, 13580, 4, 70, 6, 77904, 5, 136, 6, 442, 6, 831, 6, 186, 6, 10, 6, 4552, 6, 96704, 297, 6, 13580, 4, 2367, 6, 14135, 6, 16227, 6, 83, 5, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, -1]\n",
      "[('▁it', 0), ('▁', 0), ('▁can', 0), ('▁', 0), ('▁be', 0), ('▁', 0), ('▁a', 0), ('▁', 0), ('▁very', 0), ('▁', 0), ('▁complicat', -1), ('ed', 0), ('▁', 0), ('▁thing', 0), (',', 3), ('▁the', 0), ('▁', 0), ('▁ocean', 0), ('.', 1), ('▁and', 0), ('▁', 0), ('▁it', 0), ('▁', 0), ('▁can', 0), ('▁', 0), ('▁be', 0), ('▁', 0), ('▁a', 0), ('▁', 0), ('▁very', 0), ('▁', 0), ('▁complicat', -1), ('ed', 0), ('▁', 0), ('▁thing', 0), (',', 3), ('▁what', 0), ('▁', 0), ('▁human', 0), ('▁', 0), ('▁health', 0), ('▁', 0), ('▁is', 0), ('.', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data_path_dual = \"../dataset/en-zh-dual/\"\n",
    "os.makedirs(data_path_dual + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = map(sum, list(zip(*(return_counts(encoded_texts_zh,targets_zh),return_counts(encoded_texts_en,targets_en)))))\n",
    "space_count, p_count, q_count, comma_count"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3816779, 228107, 20588, 355804)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "encoded_texts = [encoded_texts_en[i]+encoded_texts_zh[i] for i in range(len(encoded_texts_en))]\n",
    "targets = [targets_en[i]+targets_zh[i] for i in range(len(encoded_texts_en))]\n",
    "enc = [list(zip(encoded_texts[i],targets[i])) for i in range(len(encoded_texts))]\n",
    "temp = [random.sample(enc[i], len(enc[i])) for i in range(len(encoded_texts))]\n",
    "encoded_texts = []\n",
    "targets = []\n",
    "for i in temp:\n",
    "    a,b = list(zip(*i))\n",
    "    encoded_texts.append(a)\n",
    "    targets.append(b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for i in range(0,100):\n",
    "    print(len(encoded_texts[0][i]),len(targets[0][i]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6552 6552\n",
      "6125 6125\n",
      "7686 7686\n",
      "4376 4376\n",
      "7896 7896\n",
      "724 724\n",
      "5651 5651\n",
      "3404 3404\n",
      "9993 9993\n",
      "10220 10220\n",
      "5471 5471\n",
      "4050 4050\n",
      "4825 4825\n",
      "4539 4539\n",
      "7217 7217\n",
      "5693 5693\n",
      "1280 1280\n",
      "6319 6319\n",
      "1458 1458\n",
      "5048 5048\n",
      "5652 5652\n",
      "1314 1314\n",
      "7198 7198\n",
      "5268 5268\n",
      "6420 6420\n",
      "5861 5861\n",
      "6543 6543\n",
      "6680 6680\n",
      "7071 7071\n",
      "5676 5676\n",
      "6281 6281\n",
      "7599 7599\n",
      "6721 6721\n",
      "6895 6895\n",
      "6978 6978\n",
      "767 767\n",
      "3594 3594\n",
      "4955 4955\n",
      "7110 7110\n",
      "6703 6703\n",
      "1238 1238\n",
      "1716 1716\n",
      "1149 1149\n",
      "2477 2477\n",
      "4825 4825\n",
      "1227 1227\n",
      "7223 7223\n",
      "5428 5428\n",
      "2627 2627\n",
      "6713 6713\n",
      "3954 3954\n",
      "6011 6011\n",
      "6039 6039\n",
      "3757 3757\n",
      "5739 5739\n",
      "4217 4217\n",
      "6762 6762\n",
      "4703 4703\n",
      "2699 2699\n",
      "6641 6641\n",
      "7470 7470\n",
      "3385 3385\n",
      "5544 5544\n",
      "6143 6143\n",
      "2133 2133\n",
      "2731 2731\n",
      "3083 3083\n",
      "6739 6739\n",
      "3991 3991\n",
      "807 807\n",
      "6388 6388\n",
      "6555 6555\n",
      "565 565\n",
      "2120 2120\n",
      "2374 2374\n",
      "5060 5060\n",
      "4371 4371\n",
      "5314 5314\n",
      "6263 6263\n",
      "7172 7172\n",
      "4360 4360\n",
      "7054 7054\n",
      "810 810\n",
      "3295 3295\n",
      "7335 7335\n",
      "4130 4130\n",
      "4123 4123\n",
      "4643 4643\n",
      "4185 4185\n",
      "5472 5472\n",
      "6845 6845\n",
      "4747 4747\n",
      "5866 5866\n",
      "3494 3494\n",
      "6565 6565\n",
      "7718 7718\n",
      "3390 3390\n",
      "6592 6592\n",
      "340 340\n",
      "5899 5899\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path_dual + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print(list(zip([tokenizer.convert_ids_to_tokens(token) for token in encoded_texts[0][9][0:20]],targets[0][9][0:20])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('<s>', -1), ('▁thank', 0), ('▁', 0), ('▁you', 0), ('▁', 0), ('▁so', 0), ('▁', 0), ('▁much', 0), ('▁', 0), ('▁everyone', 0), ('▁', 0), ('▁from', 0), ('▁', 0), ('▁', -1), ('ted', 0), (',', 3), ('▁and', 0), ('▁', 0), ('▁chr', -1), ('is', 0)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('mPunct': venv)"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}