{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-09 13:49:10.716690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-09 13:49:10.716716: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/malay-dataset/\"\n",
    "\n",
    "with open(data_path + 'train_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'valid_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "datasets = train_text, valid_text, test_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "[len(ds) for ds in datasets]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[23443, 79, 158]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"  ## comment this out please! no replacing '-'s for malay\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    text = re.sub(r'^[,.?]','',text) # remove all starting punctuations (they make zero sense)\n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r',\\s?\\.', '.', text) # , . -> ,\n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9]),(?=[a-zA-Z0-9])',', ',text) # say,you -> say, you\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)# ? . -> ?\n",
    "    text = re.sub(r'…+','.',text)\n",
    "    text = re.sub(r'\\,+',',',text)\n",
    "    text = re.sub(r'\\.+','.',text)\n",
    "    text = re.sub(r'\\?+','?',text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[23443, 79, 158]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[12703434, 42082, 84954]"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "target_ids = tokenizer.encode(\".?,\")[1:-1]\n",
    "target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[6, 5, 32, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'.': 5, '?': 705, ',': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[5, 705, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split()\n",
    "\n",
    "    i = 0\n",
    "    for word in words:\n",
    "\n",
    "        orig_word = word\n",
    "        word = word.encode('ascii','ignore').decode().strip()\n",
    "        \n",
    "        if len(word) == 0 or word in ['\\u200d','\\ufeff']:\n",
    "            i+=1\n",
    "            continue\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word.endswith(target_token) and word != target_token:\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target[target_id]\n",
    "            elif word == target_token:\n",
    "                target = id2target[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "\n",
    "        targets.append(0)\n",
    "        \n",
    "        if target != 0:\n",
    "            encoded_words.append(target2id[target])\n",
    "        else:\n",
    "            encoded_words.append(6)\n",
    "        \n",
    "        targets.append(target)\n",
    "        \n",
    "        \n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        if len(encoded_word) == 0:\n",
    "            print(f\"Word:  {(i, words[i], orig_word)} word: {len(word)}, {encoded_word}\")\n",
    "        assert(len(encoded_word)>0)\n",
    "        i+=1\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print(id2target)\n",
    "# s = \"Tyranosaurus: kill me? Not enough, rumplestilskin -- said the co-pilot -- ...\"\n",
    "#s = \"it  can  be  a  very  complicated  thing , the  ocean . and  it  can  be  a  very  complicated  thing, what  human  health  is.\"\n",
    "s = \"aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, tgts = create_target(s)\n",
    "print(data)\n",
    "print(tgts)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[1:-1], tgts[1:-1])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 6: 1, 5: 2, 32: 3, 4: 4}\n",
      "aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \n",
      "aku cakap ok sahaja, time itu juga dia suruh start kerja. alhamdulillah akhirnya dapat juga kerja.\n",
      "[0, 2121, 6, 55081, 6, 3664, 6, 13990, 4, 1733, 6, 752, 6, 1220, 6, 879, 6, 168753, 6, 4034, 6, 9792, 5, 193087, 6, 25375, 6, 1802, 6, 1220, 6, 9792, 5, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, -1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁aku', 0),\n",
       " ('▁', 0),\n",
       " ('▁cakap', 0),\n",
       " ('▁', 0),\n",
       " ('▁ok', 0),\n",
       " ('▁', 0),\n",
       " ('▁sahaja', 0),\n",
       " (',', 4),\n",
       " ('▁time', 0),\n",
       " ('▁', 0),\n",
       " ('▁itu', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁dia', 0),\n",
       " ('▁', 0),\n",
       " ('▁suruh', 0),\n",
       " ('▁', 0),\n",
       " ('▁start', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 2),\n",
       " ('▁alhamdulillah', 0),\n",
       " ('▁', 0),\n",
       " ('▁akhirnya', 0),\n",
       " ('▁', 0),\n",
       " ('▁dapat', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# encoded_texts, targets = create_target(transcripts[164])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# print(datasets[0][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    x = list(zip(*(create_target(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "092e6b0af91c457a9a17ad72207d6012"
      },
      "text/plain": [
       "  0%|          | 0/23443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7f5ad79668b454cb8ce6d5d8aa1a53a"
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c4ac13abc24a9d8fa51c510b6b04f8"
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "print(len(encoded_texts[1]))\n",
    "print(np.array(encoded_texts).shape, len(targets))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "79\n",
      "(3,) 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for target in targets:\n",
    "    for tar in target:\n",
    "        for ta in tar:\n",
    "            comma_count += 1 if (ta == 3) else 0\n",
    "            q_count += 1 if (ta == 2) else 0\n",
    "            p_count += 1 if (ta == 1) else 0\n",
    "   \n",
    "sc = 0\n",
    "mwc = 0\n",
    "for text,target in zip(encoded_texts, targets):\n",
    "    for tex,tar  in zip(text,target):\n",
    "        en = 0\n",
    "        for t,ta in zip(tex,tar):\n",
    "            if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                word_count+=1\n",
    "                en+=1\n",
    "            elif t in [705, 5]:\n",
    "                mwc*=sc\n",
    "                sc += 1\n",
    "                mwc += en\n",
    "                mwc /= sc\n",
    "                en = 0\n",
    "                \n",
    "print(mwc)\n",
    "     \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "            \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.4281327372574975\n",
      "635105 12802819 64487 1114889\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = return_counts(encoded_texts,targets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "\n",
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1103924\t63918\t628667\t27147171\t4503537\n",
      "3772\t179\t2057\t90056\t14771\n",
      "7193\t390\t4381\t181454\t30635\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "#print(tokenizer.decode(encoded_texts[0][1]))\n",
    "[(tokenizer.convert_ids_to_tokens(code), trgt) for code,trgt in zip(encoded_texts[0][0], targets[0][0]) if trgt not in [0,-1] ]\n",
    "#print(encoded_texts[0][0])\n",
    "# print(datasets[0][0])\n",
    "# print(encoded_texts[0][1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('▁?', 2),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " ('.', 1),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " (',', 3),\n",
       " ('.', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('mPunct': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}