{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'xlm-roberta-base' #'bert-base-bahasa-uncased' #'bert-base-multilingual-uncased' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/malay-dataset/\"\n",
    "\n",
    "with open(data_path + 'train_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'valid_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_text[0:len(train_text)//1], valid_text[0:len(valid_text)//1], test_text[0:len(test_text)//1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"  ## comment this out please! no replacing '-'s for malay\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    text = re.sub(r'^[,.?]','',text) # remove all starting punctuations (they make zero sense)\n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r',\\s?\\.', '.', text) # , . -> ,\n",
    "    text = re.sub(r'\\.\\s?,', ',', text) # . , -> ,\n",
    "    \n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9]),(?=[a-zA-Z0-9])',', ',text) # say,you -> say, you\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)# ? . -> ?\n",
    "    text = re.sub(r'…','.',text)\n",
    "    text = re.sub(r'\\,+',',',text)\n",
    "    text = re.sub(r'\\.+','.',text)\n",
    "    text = re.sub(r'\\?+','?',text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6798/6798 [00:00<00:00, 29365.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "train = re.split(r'(\\.|\\?)',datasets[0][0]) #split with capture groups\n",
    "print(len(datasets[0]))\n",
    "sentences = [[] for i in range(len(datasets[0]))]\n",
    "for j in tqdm(range(len(datasets[0]))):\n",
    "    train = re.split(r'(\\.|\\?)',datasets[0][j])\n",
    "    if train[0] == '.' or train[0] =='?':\n",
    "        print(\"OHNO \",j)\n",
    "        start = 1\n",
    "    else:\n",
    "        start = 0 \n",
    "    for i in range(start,len(train),2):\n",
    "        sentence = train[i]\n",
    "        if i < len(train)-1:\n",
    "            end = train[i+1]\n",
    "        else: \n",
    "            end = '.'\n",
    "        comma_count = sentence.count(',')\n",
    "        if comma_count >=1:\n",
    "            sentences[j].append(sentence+end)\n",
    "\n",
    "sentences = [i for i in sentences if len(i) > 0]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = \" \".join(sentences[i])\n",
    "\n",
    "datasets[0] = sentences\n",
    "## clean again\n",
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6797, 850, 850]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2780143, 480281, 504158]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 32, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids = tokenizer.encode(\".?,\")[1:-1]\n",
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 5, '?': 705, ',': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dу'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_ids\n",
    "tokenizer.decode([i for i in range(104,106)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "\n",
    "def create_target(text):\n",
    "    spaceint = 6\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split()\n",
    "\n",
    "    i = 0\n",
    "    for word in words:\n",
    "\n",
    "        orig_word = word\n",
    "        word = word.encode('ascii','ignore').decode().strip()\n",
    "        \n",
    "        if len(word) == 0 or word in ['\\u200d','\\ufeff']:\n",
    "            i+=1\n",
    "            continue\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word.endswith(target_token) and word != target_token:\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target[target_id]\n",
    "            elif word == target_token:\n",
    "                target = id2target[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "\n",
    "        targets.append(0)\n",
    "        \n",
    "        if target != 0:\n",
    "            encoded_words.append(target2id[target])\n",
    "        else:\n",
    "            encoded_words.append(spaceint)\n",
    "        \n",
    "        targets.append(target)\n",
    "        \n",
    "        \n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        if len(encoded_word) == 0:\n",
    "            print(f\"Word:  {(i, words[i], orig_word)} word: {len(word)}, {encoded_word}\")\n",
    "        assert(len(encoded_word)>0)\n",
    "        i+=1\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, -1: -1, 5: 1, 705: 2, 4: 3}\n",
      "aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \n",
      "aku cakap ok sahaja, time itu juga dia suruh start kerja. alhamdulillah akhirnya dapat juga kerja.\n",
      "[0, 2121, 6, 55081, 6, 3664, 6, 13990, 4, 1733, 6, 752, 6, 1220, 6, 879, 6, 168753, 6, 4034, 6, 9792, 5, 193087, 6, 25375, 6, 1802, 6, 1220, 6, 9792, 5, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('▁aku', 0),\n",
       " ('▁', 0),\n",
       " ('▁cakap', 0),\n",
       " ('▁', 0),\n",
       " ('▁ok', 0),\n",
       " ('▁', 0),\n",
       " ('▁sahaja', 0),\n",
       " (',', 3),\n",
       " ('▁time', 0),\n",
       " ('▁', 0),\n",
       " ('▁itu', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁dia', 0),\n",
       " ('▁', 0),\n",
       " ('▁suruh', 0),\n",
       " ('▁', 0),\n",
       " ('▁start', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 1),\n",
       " ('▁alhamdulillah', 0),\n",
       " ('▁', 0),\n",
       " ('▁akhirnya', 0),\n",
       " ('▁', 0),\n",
       " ('▁dapat', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(id2target)\n",
    "# s = \"Tyranosaurus: kill me? Not enough, rumplestilskin -- said the co-pilot -- ...\"\n",
    "#s = \"it  can  be  a  very  complicated  thing , the  ocean . and  it  can  be  a  very  complicated  thing, what  human  health  is.\"\n",
    "s = \"aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, tgts = create_target(s)\n",
    "print(data)\n",
    "print(tgts)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[1:-1], tgts[1:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_texts, targets = create_target(transcripts[164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datasets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6797/6797 [01:14<00:00, 91.80it/s] \n",
      "100%|██████████| 850/850 [00:12<00:00, 65.93it/s]\n",
      "100%|██████████| 850/850 [00:13<00:00, 63.10it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    x = list(zip(*(create_target(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "(3,) 3\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_texts[1]))\n",
    "print(np.array(encoded_texts).shape, len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.318509574504603\n",
      "265309 3764580 4235 172160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for target in targets:\n",
    "    for tar in target:\n",
    "        for ta in tar:\n",
    "            comma_count += 1 if (ta == 3) else 0\n",
    "            q_count += 1 if (ta == 2) else 0\n",
    "            p_count += 1 if (ta == 1) else 0\n",
    "   \n",
    "sc = 0\n",
    "mwc = 0\n",
    "for text,target in zip(encoded_texts, targets):\n",
    "    for tex,tar  in zip(text,target):\n",
    "        en = 0\n",
    "        for t,ta in zip(tex,tar):\n",
    "            if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                word_count+=1\n",
    "                en+=1\n",
    "            elif t in [705, 5]:\n",
    "                mwc*=sc\n",
    "                sc += 1\n",
    "                mwc += en\n",
    "                mwc /= sc\n",
    "                en = 0\n",
    "                \n",
    "print(mwc)\n",
    "     \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "            \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    print(mwc)\n",
    "    return space_count, p_count, q_count, comma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.318509574504603\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = return_counts(encoded_texts,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90470\t2506\t163939\t3856079\t622536\n",
      "18138\t915\t20750\t670571\t106877\n",
      "18103\t782\t20527\t668320\t105722\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\L12-O-~1\\AppData\\Local\\Temp/ipykernel_18164/2628654106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mraw_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.decode(encoded_texts[0][1]))\n",
    "[(tokenizer.convert_ids_to_tokens(code), trgt) for code,trgt in zip(encoded_texts[0][0], targets[0][0]) if trgt not in [0,-1] ]\n",
    "#print(encoded_texts[0][0])\n",
    "# print(datasets[0][0])\n",
    "# print(encoded_texts[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/malay-dataset/\"\n",
    "data_short = '../dataset/malay-short/'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "encd = []\n",
    "tgts = []\n",
    "with open(f\"{data_path + model_type}/train_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)\n",
    "with open(f\"{data_path + model_type}/valid_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)\n",
    "with open(f\"{data_path + model_type}/test_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "s,p,q,c = return_counts(encd,tgts)\n",
    "for i,name in enumerate(['train','valid','test']):\n",
    "    x = list(zip(encd[i],tgts[i]))\n",
    "    enc = random.sample(x,len(x)//4)\n",
    "    enc,tgt = list(zip(*enc))\n",
    "    with open(data_short+ f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((enc, tgt, s, p, q, c), f)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[250001]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_counts(encd,tgts)\n",
    "#print(encoded_texts[0][0])\n",
    "#print(tokenizer.decode(encoded_texts[0][0]))\n",
    "'''\n",
    "for word in encoded_texts[0][0]:\n",
    "    print(tokenizer.convert_ids_to_tokens(word))\n",
    "'''\n",
    "tokenizer.encode(\"<mask>\",add_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('mPunct': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
