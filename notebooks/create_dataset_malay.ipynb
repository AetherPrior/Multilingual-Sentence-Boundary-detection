{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-24 22:54:57.771980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-24 22:54:57.772009: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_type = 'xlm-roberta-base' #'bert-base-bahasa-uncased' #'bert-base-multilingual-uncased' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/malay-dataset/\"\n",
    "\n",
    "with open(data_path + 'train_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'valid_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_malay.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "datasets = [train_text[0:len(train_text)//4], valid_text[0:len(valid_text)//4], test_text[0:len(test_text)//4]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "[len(ds) for ds in datasets]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[5860, 19, 39]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"  ## comment this out please! no replacing '-'s for malay\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    text = re.sub(r'^[,.?]','',text) # remove all starting punctuations (they make zero sense)\n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '.')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r',\\s?\\.', '.', text) # , . -> ,\n",
    "    text = re.sub(r'\\.\\s?,', ',', text) # . , -> ,\n",
    "    \n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9]),(?=[a-zA-Z0-9])',', ',text) # say,you -> say, you\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)# ? . -> ?\n",
    "    text = re.sub(r'…','.',text)\n",
    "    text = re.sub(r'\\,+',',',text)\n",
    "    text = re.sub(r'\\.+','.',text)\n",
    "    text = re.sub(r'\\?+','?',text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "train = re.split(r'(\\.|\\?)',datasets[0][0]) #split with capture groups\n",
    "print(len(datasets[0]))\n",
    "sentences = [[] for i in range(len(datasets[0]))]\n",
    "for j in tqdm(range(len(datasets[0]))):\n",
    "    train = re.split(r'(\\.|\\?)',datasets[0][j])\n",
    "    if train[0] == '.' or train[0] =='?':\n",
    "        print(\"OHNO \",j)\n",
    "        start = 1\n",
    "    else:\n",
    "        start = 0 \n",
    "    for i in range(start,len(train),2):\n",
    "        sentence = train[i]\n",
    "        if i < len(train)-1:\n",
    "            end = train[i+1]\n",
    "        else: \n",
    "            end = '.'\n",
    "        comma_count = sentence.count(',')\n",
    "        if comma_count >=1:\n",
    "            sentences[j].append(sentence+end)\n",
    "\n",
    "sentences = [i for i in sentences if len(i) > 0]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = \" \".join(sentences[i])\n",
    "\n",
    "datasets[0] = sentences\n",
    "## clean again\n",
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5860\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5860/5860 [00:00<00:00, 14600.57it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[5844, 19, 39]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1599958, 10081, 20739]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "target_ids = tokenizer.encode(\".?,\")[1:-1]\n",
    "target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[6, 5, 32, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \".?,\"}\n",
    "target_token2id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'.': 5, '?': 705, ',': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_ids\n",
    "tokenizer.decode([i for i in range(104,106)])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dу'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "\n",
    "def create_target(text):\n",
    "    spaceint = 6\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split()\n",
    "\n",
    "    i = 0\n",
    "    for word in words:\n",
    "\n",
    "        orig_word = word\n",
    "        word = word.encode('ascii','ignore').decode().strip()\n",
    "        \n",
    "        if len(word) == 0 or word in ['\\u200d','\\ufeff']:\n",
    "            i+=1\n",
    "            continue\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word.endswith(target_token) and word != target_token:\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target[target_id]\n",
    "            elif word == target_token:\n",
    "                target = id2target[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "\n",
    "        targets.append(0)\n",
    "        \n",
    "        if target != 0:\n",
    "            encoded_words.append(target2id[target])\n",
    "        else:\n",
    "            encoded_words.append(spaceint)\n",
    "        \n",
    "        targets.append(target)\n",
    "        \n",
    "        \n",
    "#         print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        if len(encoded_word) == 0:\n",
    "            print(f\"Word:  {(i, words[i], orig_word)} word: {len(word)}, {encoded_word}\")\n",
    "        assert(len(encoded_word)>0)\n",
    "        i+=1\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(id2target)\n",
    "# s = \"Tyranosaurus: kill me? Not enough, rumplestilskin -- said the co-pilot -- ...\"\n",
    "#s = \"it  can  be  a  very  complicated  thing , the  ocean . and  it  can  be  a  very  complicated  thing, what  human  health  is.\"\n",
    "s = \"aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, tgts = create_target(s)\n",
    "print(data)\n",
    "print(tgts)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[1:-1], tgts[1:-1])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 5: 1, 705: 2, 4: 3}\n",
      "aku cakap ok sahaja , time itu juga dia suruh start kerja . alhamdulillah akhirnya dapat juga kerja . \n",
      "aku cakap ok sahaja, time itu juga dia suruh start kerja. alhamdulillah akhirnya dapat juga kerja.\n",
      "[0, 2121, 6, 55081, 6, 3664, 6, 13990, 4, 1733, 6, 752, 6, 1220, 6, 879, 6, 168753, 6, 4034, 6, 9792, 5, 193087, 6, 25375, 6, 1802, 6, 1220, 6, 9792, 5, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁aku', 0),\n",
       " ('▁', 0),\n",
       " ('▁cakap', 0),\n",
       " ('▁', 0),\n",
       " ('▁ok', 0),\n",
       " ('▁', 0),\n",
       " ('▁sahaja', 0),\n",
       " (',', 3),\n",
       " ('▁time', 0),\n",
       " ('▁', 0),\n",
       " ('▁itu', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁dia', 0),\n",
       " ('▁', 0),\n",
       " ('▁suruh', 0),\n",
       " ('▁', 0),\n",
       " ('▁start', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 1),\n",
       " ('▁alhamdulillah', 0),\n",
       " ('▁', 0),\n",
       " ('▁akhirnya', 0),\n",
       " ('▁', 0),\n",
       " ('▁dapat', 0),\n",
       " ('▁', 0),\n",
       " ('▁juga', 0),\n",
       " ('▁', 0),\n",
       " ('▁kerja', 0),\n",
       " ('.', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encoded_texts, targets = create_target(transcripts[164])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(datasets[0][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    x = list(zip(*(create_target(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5844/5844 [01:03<00:00, 91.82it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 46.19it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 46.65it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(len(encoded_texts[1]))\n",
    "print(np.array(encoded_texts).shape, len(targets))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19\n",
      "(3,) 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for target in targets:\n",
    "    for tar in target:\n",
    "        for ta in tar:\n",
    "            comma_count += 1 if (ta == 3) else 0\n",
    "            q_count += 1 if (ta == 2) else 0\n",
    "            p_count += 1 if (ta == 1) else 0\n",
    "   \n",
    "sc = 0\n",
    "mwc = 0\n",
    "for text,target in zip(encoded_texts, targets):\n",
    "    for tex,tar  in zip(text,target):\n",
    "        en = 0\n",
    "        for t,ta in zip(tex,tar):\n",
    "            if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                word_count+=1\n",
    "                en+=1\n",
    "            elif t in [705, 5]:\n",
    "                mwc*=sc\n",
    "                sc += 1\n",
    "                mwc += en\n",
    "                mwc /= sc\n",
    "                en = 0\n",
    "                \n",
    "print(mwc)\n",
    "     \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "            \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.314630595087122\n",
      "158653 1630456 4494 101950\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    print(mwc)\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = return_counts(encoded_texts,targets)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.314630595087122\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#print(tokenizer.decode(encoded_texts[0][1]))\n",
    "[(tokenizer.convert_ids_to_tokens(code), trgt) for code,trgt in zip(encoded_texts[0][0], targets[0][0]) if trgt not in [0,-1] ]\n",
    "#print(encoded_texts[0][0])\n",
    "# print(datasets[0][0])\n",
    "# print(encoded_texts[0][1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/malay-dataset/\"\n",
    "data_short = '../dataset/malay-short/'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "encd = []\n",
    "tgts = []\n",
    "with open(f\"{data_path + model_type}/train_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)\n",
    "with open(f\"{data_path + model_type}/valid_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)\n",
    "with open(f\"{data_path + model_type}/test_data.pkl\",'rb') as f:\n",
    "    enc, target, _,_,_,_ = pickle.load(f)\n",
    "    encd.append(enc)\n",
    "    tgts.append(target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "s,p,q,c = return_counts(encd,tgts)\n",
    "for i,name in enumerate(['train','valid','test']):\n",
    "    x = list(zip(encd[i],tgts[i]))\n",
    "    enc = random.sample(x,len(x)//4)\n",
    "    enc,tgt = list(zip(*enc))\n",
    "    with open(data_short+ f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((enc, tgt, s, p, q, c), f)\n",
    "    \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "#return_counts(encd,tgts)\n",
    "#print(encoded_texts[0][0])\n",
    "#print(tokenizer.decode(encoded_texts[0][0]))\n",
    "'''\n",
    "for word in encoded_texts[0][0]:\n",
    "    print(tokenizer.convert_ids_to_tokens(word))\n",
    "'''\n",
    "tokenizer.encode(\"<mask>\",add_special_tokens=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[250001]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('mPunct': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}