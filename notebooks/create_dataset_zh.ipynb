{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/zh-en/\"\n",
    "\n",
    "with open(data_path + 'train_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'dev_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "datasets = train_text, valid_text, test_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "[len(ds) for ds in datasets]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('！', '。')\n",
    "    text = text.replace('：', '，')\n",
    "    text = text.replace('——', '，')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s—\\s', ' ， ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '。')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace('、', '，')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('……', '')\n",
    "    text = text.replace('。”', '')\n",
    "    text = text.replace('”', '，')\n",
    "    text = text.replace('“','，')\n",
    "    text = text.replace(',','，')\n",
    "    \n",
    "\n",
    "    text = re.sub(r'——\\s?——', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r'，\\s?，', '，', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r'，\\s?。', '。', text) # , . -> ,\n",
    "    text = re.sub(r'？\\s?。', '？', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+？', '？', text)\n",
    "    text = re.sub(r'\\s+，', '，', text)\n",
    "    text = re.sub(r'。[\\s+。]+', '。 ', text)\n",
    "    text = re.sub(r'\\s+。', '。 ', text)\n",
    "    \n",
    "    #text = re.sub(r'？\\s+', '？', text)\n",
    "    #text = re.sub(r'，\\s+', '，', text)\n",
    "    #text = re.sub(r'。\\s+', '。 ', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[250445, 1968, 2860]"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "target_ids = tokenizer.encode(\"。？，\")[1:-1]\n",
    "tokenizer.convert_ids_to_tokens(target_ids)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁', '。', '?', ',']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \"。？，\"}\n",
    "target_token2id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'。': 30, '？': 705, '，': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_token2id.items()\n",
    "#target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('。', 30), ('?', 705), (',', 4)])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "import jieba\n",
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "# print(id2target, target2id)\n",
    "\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = list(jieba.cut(text,HMM=True)) ## ignore the first space\n",
    "    words2 = []\n",
    "    for i in range(len(words)):\n",
    "        encoded_word = tokenizer.encode(words[i])\n",
    "        #print(words[i],encoded_word)\n",
    "        if (len(encoded_word[1:-1]) > 1 and encoded_word[1] != 6) or (len(encoded_word[1:-1]) > 2 and encoded_word[1] == 6):\n",
    "            for word in encoded_word[1:-1]:\n",
    "                if word != 6:\n",
    "                    encoded_words.append(word)\n",
    "                    targets.append(-1)\n",
    "            targets = targets[:-1]   \n",
    "        elif len(encoded_word[1:-1]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Here! \",encoded_word)\n",
    "            s = 2 if encoded_word[1] == 6 else 1\n",
    "            encoded_words.append(encoded_word[s])\n",
    "                \n",
    "            \n",
    "            \n",
    "        if words[i] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "            if i < len(words) -1 and words[i+1] in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                ##words2.append(words[i])\n",
    "                targets.append(0)\n",
    "                pass\n",
    "            else:\n",
    "                targets.append(0)\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "        else:\n",
    "            if words[i] in [\"▁\",\" \"]:\n",
    "                if i > 0 and words[i-1] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                    #encoded_words.append(\" \")\n",
    "                    #targets.append(0)\n",
    "                    pass\n",
    "            else:\n",
    "                #print(\"YES\",words[i])\n",
    "                targets.append(id2target[target_token2id[words[i]]])\n",
    "                # words2.append(words[i])\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1]+ targets + [-1]    \n",
    "    \n",
    "    return encoded_words, targets\n",
    "\n",
    "    '''    \n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        target = 0\n",
    "        target_appended = False\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word == target_token:\n",
    "                #word = word.rstrip(target_token)\n",
    "                encoded_words.append(target_token2id[word])\n",
    "                targets.append(id2target[target_id])\n",
    "                target_appended = True\n",
    "        if not target_appended:\n",
    "            if word == ' ':\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "            else:    \n",
    "                encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "                if len(encoded_word) == 2:\n",
    "                    encoded_word = encoded_word[1:]\n",
    "\n",
    "                for w in encoded_word:\n",
    "                    encoded_words.append(w)\n",
    "\n",
    "                if len(encoded_word)>1:\n",
    "                    for _ in range(len(encoded_word)-1):\n",
    "                        if encoded_word[_] == 6:\n",
    "                            targets.append(0)\n",
    "                        else:\n",
    "                            targets.append(-1)\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(0)    \n",
    "\n",
    "#             print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "            assert(len(encoded_word)>0)\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1]+ targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets\n",
    "    '''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "print(id2target)\n",
    "s = \"谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\"\n",
    "#s = \"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\"\n",
    "#s = \"算所， 日本\"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, targets = create_target(s)\n",
    "#print(data)\n",
    "#print(targets)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[2:-1], targets[2:-1])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3}\n",
      "谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\n",
      "谁能猜一猜，你大脑里神经元的总长有多少？，西班牙厨师被控告， 非常坚硬的土地。西班牙厨师被控告\n",
      "Here!  [0, 6, 19874, 2]\n",
      "Here!  [0, 6, 1580, 2]\n",
      "Here!  [0, 6, 84952, 2]\n",
      "Here!  [0, 45690, 2]\n",
      "Here!  [0, 6, 84952, 2]\n",
      "Here!  [0, 6, 4, 2]\n",
      "Here!  [0, 73675, 2]\n",
      "Here!  [0, 6, 157938, 2]\n",
      "Here!  [0, 6, 2008, 2]\n",
      "Here!  [0, 6, 43, 2]\n",
      "Here!  [0, 6, 465, 2]\n",
      "Here!  [0, 6, 22517, 2]\n",
      "Here!  [0, 705, 2]\n",
      "Here!  [0, 6, 4, 2]\n",
      "Here!  [0, 6, 54222, 2]\n",
      "Here!  [0, 6, 1317, 2]\n",
      "Here!  [0, 6, 4, 2]\n",
      "Here!  [0, 6, 4528, 2]\n",
      "Here!  [0, 6, 43, 2]\n",
      "Here!  [0, 6, 20770, 2]\n",
      "Here!  [0, 6, 30, 2]\n",
      "Here!  [0, 6, 54222, 2]\n",
      "Here!  [0, 6, 1317, 2]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁', 0),\n",
       " ('能', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " ('▁', 0),\n",
       " ('▁一', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " (',', 3),\n",
       " ('▁你', 0),\n",
       " ('▁', 0),\n",
       " ('大脑', 0),\n",
       " ('▁', 0),\n",
       " ('里', 0),\n",
       " ('▁', 0),\n",
       " ('神经', -1),\n",
       " ('元', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('总', -1),\n",
       " ('长', 0),\n",
       " ('▁', 0),\n",
       " ('有', 0),\n",
       " ('▁', 0),\n",
       " ('多少', 0),\n",
       " ('▁?', 2),\n",
       " (',', 3),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', -1),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', -1),\n",
       " ('告', 0),\n",
       " (',', 3),\n",
       " ('非常', 0),\n",
       " ('▁', 0),\n",
       " ('坚', -1),\n",
       " ('硬', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('土地', 0),\n",
       " ('。', 1),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', -1),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', -1),\n",
       " ('告', 0),\n",
       " ('▁', 0)]"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# sentence endings split\n",
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    trgts = []\n",
    "    for ts in ds:\n",
    "        prev = 0\n",
    "        init = 0\n",
    "        #print(\"Length of sequence: \",len(ts))\n",
    "        for i in range(len(ts)):\n",
    "            if  ts[i] in [\"。\",\".\",\"？\",\"?\"]:\n",
    "                if i > init+511:\n",
    "                    if prev == 0:\n",
    "                        #print(\"truncating first sentence\")\n",
    "                        trgts.append(ts[0:512])\n",
    "                        prev = 511\n",
    "                        init = 511\n",
    "                    else:\n",
    "                        if prev == init: \n",
    "                            prev = i\n",
    "                        #print(\"appending from \",init,\" to \",prev)\n",
    "                        if prev - init > 511:\n",
    "                            #print(\"CHUNKing sentence\")\n",
    "                            ls = ts[init+1:prev+1]\n",
    "                            trgts.extend([ls[i:i+512] for i in range(0,prev-init,512)])\n",
    "                        else:\n",
    "                            trgts.append(ts[init+1:prev+1])\n",
    "                        init = prev\n",
    "                        prev = init\n",
    "                else:\n",
    "                    prev = i\n",
    "        \n",
    "        if prev < len(ts)-1:\n",
    "            #print(\"appending last sentence from \",prev,\" to \",len(ts)-1)\n",
    "            #if(len(ts)-1 - prev > 511):\n",
    "            #    #print(\"chunking last sentence\")\n",
    "            trgts.extend([ts[i:i+512] for i in range(prev,len(ts)-1,512)])\n",
    "            #trgts.append(ts[prev:len(ts)])\n",
    "    x = list(zip(*(create_target(trgt) for trgt in tqdm(trgts))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/9486 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e7ce40f79364332b34d994b6d175df2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5be34fcd3394a7a9d74785572273a34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b62eb2275244ca84e87b94d3c6d19c"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## basic split\n",
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    trgts = []\n",
    "    for ts in ds:\n",
    "        trgts.extend([ts[i:i+512] for i in range(0,len(ts),512)])\n",
    "    x = list(zip(*(create_target(trgt) for trgt in tqdm(trgts))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for target in targets:\n",
    "    for tar in target:\n",
    "        for ta in tar:\n",
    "            comma_count += 1 if (ta == 3) else 0\n",
    "            q_count += 1 if (ta == 2) else 0\n",
    "            p_count += 1 if (ta == 1) else 0\n",
    "\n",
    "sc = 0\n",
    "mwc = 0\n",
    "for text in encoded_texts:\n",
    "    for tex in text:\n",
    "        en = 0\n",
    "        for t in tex:\n",
    "            if t not in [6,30,0,-1,1,2,4,705]:\n",
    "                word_count+=1\n",
    "                en+=1\n",
    "            elif t in [705, 30, 4]:\n",
    "                mwc*=sc\n",
    "                sc += 1\n",
    "                mwc += en\n",
    "                mwc /= sc\n",
    "                en = 0\n",
    "                \n",
    "print(mwc)\n",
    "                \n",
    "        \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "\n",
    "           \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8.816082301070676\n",
      "147063 2193867 10339 80639\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = return_counts(encoded_texts,targets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9004\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "139619\t10215\t188165\t2001462\t267423\n",
      "909\t71\t1225\t15141\t1899\n",
      "1100\t46\t1120\t16208\t2072\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "#print(tokenizer.decode([30,4,5 ,\n",
    "#                        30]))\n",
    "#print([np.unique(i) for i in encoded_texts[1]])\n",
    "print(tokenizer.decode(encoded_texts[1][2]), len(encoded_texts[1][2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>这个 项目 叫  photo  syn  th,它 实际上 融合 了 两个 不同的 技术,一个 是  se  ad ragon, 而 另一个 则是 源 自 华盛顿 大学 的 研究生  no  ah  sna  ve  ly 所 进行 的 计算机 视觉 研究 的 成果。这项 研究 还 得到了 华盛顿 大学  ste  ve  se itz 和 微软 研究院 rick szel ki 的 协助。这是一个 非常 漂亮的 合作 成果。这个 项目  在 互联网 上 已经 得到 应用 了,它是 基于  se  ad ragon 技术 构建 的。 你 可以看到, 我们 轻松 地 对 图片 进行 多种 方式 的 查看,从而 能够 对 图片 进行 细致 的 剖 析 并且 拥有 多 分辨 率 的 浏览 体验。 不过,这些 图片  在  三 维 空间 的 排 列 事实上 是非常 有 意义 的。计算机 视觉 算法 将 这些 图片 联系 到 一起,那么 这些 图片 就 能够 将 真实 空间 呈现 出来 了, 而  我们 正是 在这个 空间 里 拍 下了 上述 的照片,这些 照片 都是  在 加拿大 落 基 山 脉 的 格 拉 西 湖  (  gra si  la  kes  ) 附近 拍 下的, ( 所有 照片  ) 都是 在这里 拍 下的。因此  你 可以看到 这里的 元素 是 稳定的 幻 灯 放映 或者 全 景 成 像, 而 这些 内容  在 空间 上 都是 关联 的。我不 确定  我们 是否有 时间 来 展示 更多的 环境 全 景。有很多 例子 比 这个 的 空间 感 还要 强。</s> 460\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('mPunct': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}