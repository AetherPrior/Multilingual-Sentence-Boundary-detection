{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/zh-en/\"\n",
    "\n",
    "with open(data_path + 'train_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'dev_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "datasets = train_text, valid_text, test_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "[len(ds) for ds in datasets]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('！', '。')\n",
    "    text = text.replace('：', '，')\n",
    "    text = text.replace('——', '，')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s—\\s', ' ， ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '。')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace('、', '，')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('……', '')\n",
    "    text = text.replace('。”', '')\n",
    "    text = text.replace('”', '，')\n",
    "    text = text.replace('“','，')\n",
    "    text = text.replace(',','，')\n",
    "    \n",
    "\n",
    "    text = re.sub(r'——\\s?——', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r'，\\s?，', '，', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r'，\\s?。', '。', text) # , . -> ,\n",
    "    text = re.sub(r'？\\s?。', '？', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+？', '？', text)\n",
    "    text = re.sub(r'\\s+，', '，', text)\n",
    "    text = re.sub(r'。[\\s+。]+', '。 ', text)\n",
    "    text = re.sub(r'\\s+。', '。 ', text)\n",
    "    \n",
    "    #text = re.sub(r'？\\s+', '？', text)\n",
    "    #text = re.sub(r'，\\s+', '，', text)\n",
    "    #text = re.sub(r'。\\s+', '。 ', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[316676, 2307, 3608]"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "target_ids = tokenizer.encode(\"。？，\")[1:-1]\n",
    "tokenizer.convert_ids_to_tokens(target_ids)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁', '。', '?', ',']"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \"。？，\"}\n",
    "target_token2id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'。': 30, '？': 705, '，': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_token2id.items()\n",
    "#target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('。', 30), ('？', 705), ('，', 4)])"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "import jieba\n",
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "# print(id2target, target2id)\n",
    "\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = list(jieba.cut(text,HMM=True)) ## ignore the first space\n",
    "    words2 = []\n",
    "    for i in range(len(words)):\n",
    "        encoded_word = tokenizer.encode(words[i])\n",
    "        #print(words[i],encoded_word)\n",
    "        if (len(encoded_word[1:-1]) > 1 and encoded_word[1] != 6) or (len(encoded_word[1:-1]) > 2 and encoded_word[1] == 6):\n",
    "            for word in encoded_word[1:-1]:\n",
    "                if word != 6:\n",
    "                    encoded_words.append(word)\n",
    "                    targets.append(-1)\n",
    "            targets = targets[:-1]   \n",
    "        elif len(encoded_word[1:-1]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            #print(\"Here! \",encoded_word)\n",
    "            s = 2 if encoded_word[1] == 6 else 1\n",
    "            encoded_words.append(encoded_word[s])\n",
    "                \n",
    "            \n",
    "            \n",
    "        if words[i] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "            if i < len(words) -1 and words[i+1] in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                ##words2.append(words[i])\n",
    "                targets.append(0)\n",
    "                pass\n",
    "            else:\n",
    "                targets.append(0)\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "        else:\n",
    "            if words[i] in [\"▁\",\" \"]:\n",
    "                if i > 0 and words[i-1] not in [\"。\",\"？\",\"，\",\" \",\"▁\"]:\n",
    "                    #encoded_words.append(\" \")\n",
    "                    #targets.append(0)\n",
    "                    pass\n",
    "            else:\n",
    "                #print(\"YES\",words[i])\n",
    "                targets.append(id2target[target_token2id[words[i]]])\n",
    "                # words2.append(words[i])\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1]+ targets + [-1]    \n",
    "    \n",
    "    return encoded_words, targets\n",
    "\n",
    "    '''    \n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        target = 0\n",
    "        target_appended = False\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word == target_token:\n",
    "                #word = word.rstrip(target_token)\n",
    "                encoded_words.append(target_token2id[word])\n",
    "                targets.append(id2target[target_id])\n",
    "                target_appended = True\n",
    "        if not target_appended:\n",
    "            if word == ' ':\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "            else:    \n",
    "                encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "                if len(encoded_word) == 2:\n",
    "                    encoded_word = encoded_word[1:]\n",
    "\n",
    "                for w in encoded_word:\n",
    "                    encoded_words.append(w)\n",
    "\n",
    "                if len(encoded_word)>1:\n",
    "                    for _ in range(len(encoded_word)-1):\n",
    "                        if encoded_word[_] == 6:\n",
    "                            targets.append(0)\n",
    "                        else:\n",
    "                            targets.append(-1)\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(0)    \n",
    "\n",
    "#             print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "            assert(len(encoded_word)>0)\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1]+ targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets\n",
    "    '''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "print(id2target)\n",
    "s = \"谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\"\n",
    "#s = \"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\"\n",
    "#s = \"算所， 日本\"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, targets = create_target(s)\n",
    "#print(data)\n",
    "#print(targets)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[2:-1], targets[2:-1])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3}\n",
      "谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\n",
      "谁能猜一猜，你大脑里神经元的总长有多少？，西班牙厨师被控告， 非常坚硬的土地。西班牙厨师被控告\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁', 0),\n",
       " ('能', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " ('▁', 0),\n",
       " ('▁一', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " (',', 3),\n",
       " ('▁你', 0),\n",
       " ('▁', 0),\n",
       " ('大脑', 0),\n",
       " ('▁', 0),\n",
       " ('里', 0),\n",
       " ('▁', 0),\n",
       " ('神经', -1),\n",
       " ('元', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('总', -1),\n",
       " ('长', 0),\n",
       " ('▁', 0),\n",
       " ('有', 0),\n",
       " ('▁', 0),\n",
       " ('多少', 0),\n",
       " ('▁?', 2),\n",
       " (',', 3),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', -1),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', -1),\n",
       " ('告', 0),\n",
       " (',', 3),\n",
       " ('非常', 0),\n",
       " ('▁', 0),\n",
       " ('坚', -1),\n",
       " ('硬', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('土地', 0),\n",
       " ('。', 1),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', -1),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', -1),\n",
       " ('告', 0),\n",
       " ('▁', 0)]"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# sentence endings split\n",
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    trgts = []\n",
    "    for ts in ds:\n",
    "        prev = 0\n",
    "        init = 0\n",
    "        #print(\"Length of sequence: \",len(ts))\n",
    "        for i in range(len(ts)):\n",
    "            if  ts[i] in [\"。\",\".\",\"？\",\"?\"]:\n",
    "                if i > init+511:\n",
    "                    if prev == 0:\n",
    "                        #print(\"truncating first sentence\")\n",
    "                        trgts.append(ts[0:512])\n",
    "                        prev = 511\n",
    "                        init = 511\n",
    "                    else:\n",
    "                        if prev == init: \n",
    "                            prev = i\n",
    "                        #print(\"appending from \",init,\" to \",prev)\n",
    "                        if prev - init > 511:\n",
    "                            #print(\"CHUNKing sentence\")\n",
    "                            ls = ts[init+1:prev+1]\n",
    "                            trgts.extend([ls[i:i+512] for i in range(0,prev-init,512)])\n",
    "                        else:\n",
    "                            trgts.append(ts[init+1:prev+1])\n",
    "                        init = prev\n",
    "                        prev = init\n",
    "                else:\n",
    "                    prev = i\n",
    "        \n",
    "        if prev < len(ts)-1:\n",
    "            #print(\"appending last sentence from \",prev,\" to \",len(ts)-1)\n",
    "            #if(len(ts)-1 - prev > 511):\n",
    "            #    #print(\"chunking last sentence\")\n",
    "            trgts.extend([ts[i:i+512] for i in range(prev,len(ts)-1,512)])\n",
    "            #trgts.append(ts[prev:len(ts)])\n",
    "    x = list(zip(*(create_target(trgt) for trgt in tqdm(trgts))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b80485aabc2544db93e75e8e57bd947d"
      },
      "text/plain": [
       "  0%|          | 0/9357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e28bd0c112c47e483a7c4a6bb64b231"
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31b8e76813a64f8abef7953e511a3a2d"
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## basic split\n",
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    trgts = []\n",
    "    for ts in ds:\n",
    "        trgts.extend([ts[i:i+512] for i in range(0,len(ts),512)])\n",
    "    x = list(zip(*(create_target(trgt) for trgt in tqdm(trgts))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "#no split\n",
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    x = list(zip(*(create_target(ts) for ts in tqdm(ds))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cac606e47024eb591a751bdbc4919ca"
      },
      "text/plain": [
       "  0%|          | 0/1017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6406c39fd7904aabb365cdf777bf3ce0"
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ebc5827da694c6885fef04bd614780a"
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for target in targets[:]:\n",
    "    for tar in target:\n",
    "        for ta in tar:\n",
    "            comma_count += 1 if (ta == 3) else 0\n",
    "            q_count += 1 if (ta == 2) else 0\n",
    "            p_count += 1 if (ta == 1) else 0\n",
    "\n",
    "sc = 0\n",
    "mwc = 0\n",
    "for text in encoded_texts[:]:\n",
    "    for tex in text:\n",
    "        en = 0\n",
    "        for t in tex:\n",
    "            if t not in [6,30,0,-1,1,2,4,705]:\n",
    "                word_count+=1\n",
    "                en+=1\n",
    "            elif t in [705, 30, 4]:\n",
    "                mwc*=sc\n",
    "                sc += 1\n",
    "                mwc += en\n",
    "                mwc /= sc\n",
    "                en = 0\n",
    "                \n",
    "print(mwc)\n",
    "                \n",
    "        \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "\n",
    "           \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.086594393462324\n",
      "157004 2591643 10256 86479\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "def return_counts(encoded_texts, targets):\n",
    "    # encoded_words, targets\n",
    "    comma_count = 0\n",
    "    word_count = 0\n",
    "    q_count = 0\n",
    "    p_count = 0\n",
    "    space_count = 0\n",
    "    for target in targets:\n",
    "        for tar in target:\n",
    "            for ta in tar:\n",
    "                comma_count += 1 if (ta == 3) else 0\n",
    "                q_count += 1 if (ta == 2) else 0\n",
    "                p_count += 1 if (ta == 1) else 0\n",
    "    sc = 0\n",
    "    mwc = 0\n",
    "    for text,target in zip(encoded_texts, targets):\n",
    "        for tex,tar  in zip(text,target):\n",
    "            en = 0\n",
    "            for t,ta in zip(tex,tar):\n",
    "                if t not in [6,5,0,-1,1,2,4,705] and ta != -1:\n",
    "                    word_count+=1\n",
    "                    en+=1\n",
    "                elif t == 6 and ta != -1: # space\n",
    "                    space_count+=1\n",
    "                elif t in [705, 5]:\n",
    "                    mwc*=sc\n",
    "                    sc += 1\n",
    "                    mwc += en\n",
    "                    mwc /= sc\n",
    "                    en = 0\n",
    "    return space_count, p_count, q_count, comma_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "space_count, p_count, q_count, comma_count = return_counts(encoded_texts,targets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i], space_count, p_count, q_count, comma_count), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9004\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80006\t9499\t144329\t3701035\t373362\n",
      "395\t88\t1151\t32002\t3611\n",
      "848\t144\t2067\t49241\t4530\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "#print(tokenizer.decode([30,4,5 ,\n",
    "#                        30]))\n",
    "#print([np.unique(i) for i in encoded_texts[1]])\n",
    "print(tokenizer.decode(encoded_texts[1][2]), targets[1][2])\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(encoded_texts[1][2], targets[1][2])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>这个 项目 叫  photosynth,它 实际上 融合 了 两个 不同 的 技术,一个 是  seadragon, 而 另 一个 则 是 源自 华盛顿大学 的 研究生  noah snavely所 进行 的 计算机 视觉 研究 的 成果。这项 研究 还 得到 了 华盛顿大学  steve seitz和 微软 研究院 rickszeliski 的 协助。这是 一个 非常 漂亮 的 合作 成果。这个 项目  在 互联网 上 已经 得到 应用 了,它 是 基于  seadragon 技术 构建 的。 你 可以 看到, 我们 轻松 地 对 图片 进行 多种 方式 的 查看,从而 能够 对 图片 进行 细致 的 剖析并且 拥有 多 分辨率 的 浏览 体验。 不过,这些 图片  在  三维空间 的 排列 事实上 是 非常 有 意义 的。计算机 视觉 算法 将 这些 图片 联系 到 一起,那么 这些 图片 就 能够 将 真实 空间 呈现 出来 了, 而  我们 正是  在 这个 空间 里 拍 下 了 上述 的 照片,这些 照片 都 是  在加拿大 落基 山脉 的 格拉西 湖  (  grassi lakes  ) 附近 拍下 的, ( 所有 照片  ) 都 是  在 这里 拍下 的。因此  你 可以 看到 这里 的 元素 是 稳定 的 幻灯 放映 或者 全景 成像, 而 这些 内容  在 空间 上 都 是 关联 的。 我 不 确定  我们 是否 有 时间 来 展示 更 多 的 环境 全景。有 很多 例子 比 这个 的 空间感 还要 强。</s> [-1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, -1, -1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, -1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('<s>', -1),\n",
       " ('这个', 0),\n",
       " ('▁', 0),\n",
       " ('项目', 0),\n",
       " ('▁', 0),\n",
       " ('叫', 0),\n",
       " ('▁', 0),\n",
       " ('▁photos', -1),\n",
       " ('yn', -1),\n",
       " ('th', 0),\n",
       " (',', 3),\n",
       " ('它', 0),\n",
       " ('▁', 0),\n",
       " ('实际上', 0),\n",
       " ('▁', 0),\n",
       " ('融合', 0),\n",
       " ('▁', 0),\n",
       " ('了', 0),\n",
       " ('▁', 0),\n",
       " ('两个', 0),\n",
       " ('▁', 0),\n",
       " ('不同', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('技术', 0),\n",
       " (',', 3),\n",
       " ('一个', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('▁se', -1),\n",
       " ('ad', -1),\n",
       " ('ragon', 0),\n",
       " (',', 3),\n",
       " ('▁而', 0),\n",
       " ('▁', 0),\n",
       " ('另', 0),\n",
       " ('▁', 0),\n",
       " ('一个', 0),\n",
       " ('▁', 0),\n",
       " ('则', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('源', -1),\n",
       " ('自', 0),\n",
       " ('▁', 0),\n",
       " ('华盛顿', -1),\n",
       " ('大学', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('研究生', 0),\n",
       " ('▁', 0),\n",
       " ('▁no', -1),\n",
       " ('ah', 0),\n",
       " ('▁sna', -1),\n",
       " ('ve', -1),\n",
       " ('ly', 0),\n",
       " ('所', 0),\n",
       " ('▁', 0),\n",
       " ('进行', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('计算机', 0),\n",
       " ('▁', 0),\n",
       " ('视觉', 0),\n",
       " ('▁', 0),\n",
       " ('研究', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('成果', 0),\n",
       " ('。', 1),\n",
       " ('这项', 0),\n",
       " ('▁', 0),\n",
       " ('研究', 0),\n",
       " ('▁', 0),\n",
       " ('还', 0),\n",
       " ('▁', 0),\n",
       " ('得到', 0),\n",
       " ('▁', 0),\n",
       " ('了', 0),\n",
       " ('▁', 0),\n",
       " ('华盛顿', -1),\n",
       " ('大学', 0),\n",
       " ('▁', 0),\n",
       " ('▁ste', -1),\n",
       " ('ve', 0),\n",
       " ('▁se', -1),\n",
       " ('itz', 0),\n",
       " ('和', 0),\n",
       " ('▁', 0),\n",
       " ('微软', 0),\n",
       " ('▁', 0),\n",
       " ('研究院', 0),\n",
       " ('▁', 0),\n",
       " ('rick', 0),\n",
       " ('szel', -1),\n",
       " ('iski', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('协助', 0),\n",
       " ('。', 1),\n",
       " ('这是', 0),\n",
       " ('▁', 0),\n",
       " ('一个', 0),\n",
       " ('▁', 0),\n",
       " ('非常', 0),\n",
       " ('▁', 0),\n",
       " ('漂亮', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('合作', 0),\n",
       " ('▁', 0),\n",
       " ('成果', 0),\n",
       " ('。', 1),\n",
       " ('这个', 0),\n",
       " ('▁', 0),\n",
       " ('项目', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('▁', 0),\n",
       " ('互联网', 0),\n",
       " ('▁', 0),\n",
       " ('上', 0),\n",
       " ('▁', 0),\n",
       " ('已经', 0),\n",
       " ('▁', 0),\n",
       " ('得到', 0),\n",
       " ('▁', 0),\n",
       " ('应用', 0),\n",
       " ('▁', 0),\n",
       " ('了', 0),\n",
       " (',', 3),\n",
       " ('它', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('基于', 0),\n",
       " ('▁', 0),\n",
       " ('▁se', -1),\n",
       " ('ad', -1),\n",
       " ('ragon', 0),\n",
       " ('▁', 0),\n",
       " ('技术', 0),\n",
       " ('▁', 0),\n",
       " ('构建', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('。', 1),\n",
       " ('▁你', 0),\n",
       " ('▁', 0),\n",
       " ('可以', 0),\n",
       " ('▁', 0),\n",
       " ('看到', 0),\n",
       " (',', 3),\n",
       " ('▁我们', 0),\n",
       " ('▁', 0),\n",
       " ('轻松', 0),\n",
       " ('▁', 0),\n",
       " ('地', 0),\n",
       " ('▁', 0),\n",
       " ('对', 0),\n",
       " ('▁', 0),\n",
       " ('图片', 0),\n",
       " ('▁', 0),\n",
       " ('进行', 0),\n",
       " ('▁', 0),\n",
       " ('多种', 0),\n",
       " ('▁', 0),\n",
       " ('方式', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('查看', 0),\n",
       " (',', 3),\n",
       " ('从而', 0),\n",
       " ('▁', 0),\n",
       " ('能够', 0),\n",
       " ('▁', 0),\n",
       " ('对', 0),\n",
       " ('▁', 0),\n",
       " ('图片', 0),\n",
       " ('▁', 0),\n",
       " ('进行', 0),\n",
       " ('▁', 0),\n",
       " ('细致', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('剖', -1),\n",
       " ('析', 0),\n",
       " ('并且', 0),\n",
       " ('▁', 0),\n",
       " ('拥有', 0),\n",
       " ('▁', 0),\n",
       " ('多', 0),\n",
       " ('▁', 0),\n",
       " ('分辨', -1),\n",
       " ('率', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('浏览', 0),\n",
       " ('▁', 0),\n",
       " ('体验', 0),\n",
       " ('。', 1),\n",
       " ('▁不过', 0),\n",
       " (',', 3),\n",
       " ('这些', 0),\n",
       " ('▁', 0),\n",
       " ('图片', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('▁', 0),\n",
       " ('▁三', -1),\n",
       " ('维', -1),\n",
       " ('空间', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('排', -1),\n",
       " ('列', 0),\n",
       " ('▁', 0),\n",
       " ('事实上', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('非常', 0),\n",
       " ('▁', 0),\n",
       " ('有', 0),\n",
       " ('▁', 0),\n",
       " ('意义', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('。', 1),\n",
       " ('计算机', 0),\n",
       " ('▁', 0),\n",
       " ('视觉', 0),\n",
       " ('▁', 0),\n",
       " ('算法', 0),\n",
       " ('▁', 0),\n",
       " ('将', 0),\n",
       " ('▁', 0),\n",
       " ('这些', 0),\n",
       " ('▁', 0),\n",
       " ('图片', 0),\n",
       " ('▁', 0),\n",
       " ('联系', 0),\n",
       " ('▁', 0),\n",
       " ('到', 0),\n",
       " ('▁', 0),\n",
       " ('一起', 0),\n",
       " (',', 3),\n",
       " ('那么', 0),\n",
       " ('▁', 0),\n",
       " ('这些', 0),\n",
       " ('▁', 0),\n",
       " ('图片', 0),\n",
       " ('▁', 0),\n",
       " ('就', 0),\n",
       " ('▁', 0),\n",
       " ('能够', 0),\n",
       " ('▁', 0),\n",
       " ('将', 0),\n",
       " ('▁', 0),\n",
       " ('真实', 0),\n",
       " ('▁', 0),\n",
       " ('空间', 0),\n",
       " ('▁', 0),\n",
       " ('呈现', 0),\n",
       " ('▁', 0),\n",
       " ('出来', 0),\n",
       " ('▁', 0),\n",
       " ('了', 0),\n",
       " (',', 3),\n",
       " ('▁而', 0),\n",
       " ('▁', 0),\n",
       " ('▁我们', 0),\n",
       " ('▁', 0),\n",
       " ('正是', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('▁', 0),\n",
       " ('这个', 0),\n",
       " ('▁', 0),\n",
       " ('空间', 0),\n",
       " ('▁', 0),\n",
       " ('里', 0),\n",
       " ('▁', 0),\n",
       " ('拍', 0),\n",
       " ('▁', 0),\n",
       " ('下', 0),\n",
       " ('▁', 0),\n",
       " ('了', 0),\n",
       " ('▁', 0),\n",
       " ('上述', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('照片', 0),\n",
       " (',', 3),\n",
       " ('这些', 0),\n",
       " ('▁', 0),\n",
       " ('照片', 0),\n",
       " ('▁', 0),\n",
       " ('都', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('加拿大', 0),\n",
       " ('▁', 0),\n",
       " ('落', -1),\n",
       " ('基', 0),\n",
       " ('▁', 0),\n",
       " ('山', -1),\n",
       " ('脉', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('格', -1),\n",
       " ('拉', -1),\n",
       " ('西', 0),\n",
       " ('▁', 0),\n",
       " ('湖', 0),\n",
       " ('▁', 0),\n",
       " ('▁(', 0),\n",
       " ('▁', 0),\n",
       " ('▁gras', -1),\n",
       " ('si', 0),\n",
       " ('▁la', -1),\n",
       " ('kes', 0),\n",
       " ('▁', 0),\n",
       " ('▁)', 0),\n",
       " ('▁', 0),\n",
       " ('附近', 0),\n",
       " ('▁', 0),\n",
       " ('拍', -1),\n",
       " ('下', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " (',', 3),\n",
       " ('▁(', 0),\n",
       " ('▁', 0),\n",
       " ('所有', 0),\n",
       " ('▁', 0),\n",
       " ('照片', 0),\n",
       " ('▁', 0),\n",
       " ('▁)', 0),\n",
       " ('▁', 0),\n",
       " ('都', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('▁', 0),\n",
       " ('这里', 0),\n",
       " ('▁', 0),\n",
       " ('拍', -1),\n",
       " ('下', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('。', 1),\n",
       " ('因此', 0),\n",
       " ('▁', 0),\n",
       " ('▁你', 0),\n",
       " ('▁', 0),\n",
       " ('可以', 0),\n",
       " ('▁', 0),\n",
       " ('看到', 0),\n",
       " ('▁', 0),\n",
       " ('这里', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('元素', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('稳定', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('幻', -1),\n",
       " ('灯', 0),\n",
       " ('▁', 0),\n",
       " ('放映', 0),\n",
       " ('▁', 0),\n",
       " ('或者', 0),\n",
       " ('▁', 0),\n",
       " ('全', -1),\n",
       " ('景', 0),\n",
       " ('▁', 0),\n",
       " ('成', -1),\n",
       " ('像', 0),\n",
       " (',', 3),\n",
       " ('▁而', 0),\n",
       " ('▁', 0),\n",
       " ('这些', 0),\n",
       " ('▁', 0),\n",
       " ('内容', 0),\n",
       " ('▁', 0),\n",
       " ('▁在', 0),\n",
       " ('▁', 0),\n",
       " ('空间', 0),\n",
       " ('▁', 0),\n",
       " ('上', 0),\n",
       " ('▁', 0),\n",
       " ('都', 0),\n",
       " ('▁', 0),\n",
       " ('是', 0),\n",
       " ('▁', 0),\n",
       " ('关联', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('。', 1),\n",
       " ('▁我', 0),\n",
       " ('▁', 0),\n",
       " ('不', 0),\n",
       " ('▁', 0),\n",
       " ('确定', 0),\n",
       " ('▁', 0),\n",
       " ('▁我们', 0),\n",
       " ('▁', 0),\n",
       " ('是否', 0),\n",
       " ('▁', 0),\n",
       " ('有', 0),\n",
       " ('▁', 0),\n",
       " ('时间', 0),\n",
       " ('▁', 0),\n",
       " ('来', 0),\n",
       " ('▁', 0),\n",
       " ('展示', 0),\n",
       " ('▁', 0),\n",
       " ('更', 0),\n",
       " ('▁', 0),\n",
       " ('多', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('环境', 0),\n",
       " ('▁', 0),\n",
       " ('全', -1),\n",
       " ('景', 0),\n",
       " ('。', 1),\n",
       " ('有', 0),\n",
       " ('▁', 0),\n",
       " ('很多', 0),\n",
       " ('▁', 0),\n",
       " ('例子', 0),\n",
       " ('▁', 0),\n",
       " ('比', 0),\n",
       " ('▁', 0),\n",
       " ('这个', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('空间', -1),\n",
       " ('感', 0),\n",
       " ('▁', 0),\n",
       " ('还要', 0),\n",
       " ('▁', 0),\n",
       " ('强', 0),\n",
       " ('。', 1),\n",
       " ('</s>', -1)]"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('mPunct': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}