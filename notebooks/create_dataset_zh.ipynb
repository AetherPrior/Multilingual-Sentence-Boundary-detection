{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model_type = 'xlm-roberta-base' #albert-base-v1, bert-base-cased, bert-base-uncased\n",
    "data_path = \"../dataset/zh-en/\"\n",
    "\n",
    "with open(data_path + 'train_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(data_path + 'dev_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    valid_text = f.readlines()\n",
    "with open(data_path + 'test_texts_zh.txt', 'r', encoding='utf-8') as f:\n",
    "    test_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "datasets = train_text, valid_text, test_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "[len(ds) for ds in datasets]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('！', '。')\n",
    "    text = text.replace('：', '，')\n",
    "    text = text.replace('——', '，')\n",
    "    \n",
    "    #reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    #r = re.compile(reg, re.DOTALL)\n",
    "    #text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s—\\s', ' ， ', text)\n",
    "    \n",
    "#     text = text.replace('-', ',')\n",
    "    text = text.replace(';', '。')    # replace symbols with the most relevant counterparts\n",
    "    text = text.replace('、', '，')\n",
    "    text = text.replace('♫', '')\n",
    "    text = text.replace('……', '')\n",
    "    text = text.replace('。”', '')\n",
    "    text = text.replace('”', '，')\n",
    "    text = text.replace('“','，')\n",
    "    text = text.replace(',','，')\n",
    "    \n",
    "\n",
    "    text = re.sub(r'——\\s?——', '', text) # replace --   -- to ''\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all whitespaces\n",
    "    \n",
    "    text = re.sub(r'，\\s?，', '，', text)  # merge commas separating only whitespace\n",
    "    text = re.sub(r'，\\s?。', '。', text) # , . -> ,\n",
    "    text = re.sub(r'？\\s?。', '？', text)# ? . -> ?\n",
    "    text = re.sub(r'\\s+', ' ', text)    # strip all redundant whitespace that could have been caused by preprocessing\n",
    "    \n",
    "    text = re.sub(r'\\s+？', '？', text)\n",
    "    text = re.sub(r'\\s+，', '，', text)\n",
    "    text = re.sub(r'。[\\s+。]+', '。 ', text)\n",
    "    text = re.sub(r'\\s+。', '。 ', text)\n",
    "    \n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "datasets = [[clean_text(text) for text in ds] for ds in datasets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "[len([t for t in ds if len(t)>0]) for ds in datasets] # remove all 0 word datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1017, 8, 11]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "[len(' '.join(ds).split(' ')) for ds in datasets] # make them sentences separated by a space for tokenizing"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[316676, 2307, 3608]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "target_ids = tokenizer.encode(\"。？，\")[1:-1]\n",
    "tokenizer.convert_ids_to_tokens(target_ids)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁', '。', '?', ',']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \"。?,\"}\n",
    "target_token2id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'。': 30, '?': 705, ',': 4}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "target_ids = list(target_token2id.values())\n",
    "target_token2id.items()\n",
    "#target_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('。', 30), ('?', 705), (',', 4)])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "print(id2target, target2id)\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = tokenizer.tokenize(text)[1:] ## ignore the first space\n",
    "\n",
    "    words2 = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "            if i < len(words) -1 and words[i+1] in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "                words2.append(words[i])\n",
    "            else:\n",
    "                words2.extend([words[i],' '])\n",
    "        else:\n",
    "            if words[i] == \"▁\":\n",
    "                if i > 0 and words[i-1] not in [\"。\",\"?\",\",\",\" \",\"▁\"]:\n",
    "                    words2.append(\" \")\n",
    "            else:\n",
    "                words2.append(words[i])\n",
    "            \n",
    "    words = words2\n",
    "    \n",
    "    for word in words:\n",
    "        target = 0\n",
    "        target_appended = False\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word == target_token:\n",
    "                #word = word.rstrip(target_token)\n",
    "                encoded_words.append(target_token2id[word])\n",
    "                targets.append(id2target[target_id])\n",
    "                target_appended = True\n",
    "        if not target_appended:\n",
    "            if word == ' ':\n",
    "                encoded_words.append(6)\n",
    "                targets.append(0)\n",
    "            else:    \n",
    "                encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "                if len(encoded_word) == 2:\n",
    "                    encoded_word = encoded_word[1:]\n",
    "\n",
    "                for w in encoded_word:\n",
    "                    encoded_words.append(w)\n",
    "\n",
    "                if len(encoded_word)>1:\n",
    "                    for _ in range(len(encoded_word)-1):\n",
    "                        if encoded_word[_] == 6:\n",
    "                            targets.append(0)\n",
    "                        else:\n",
    "                            targets.append(-1)\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(0)    \n",
    "\n",
    "#             print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "            assert(len(encoded_word)>0)\n",
    "    \n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    \n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3} {0: 0, -1: -1, 1: 30, 2: 705, 3: 4}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "print(id2target)\n",
    "s = \"谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\"\n",
    "print(s)\n",
    "s = clean_text(s)\n",
    "print(s)\n",
    "data, targets = create_target(s)\n",
    "print(data)\n",
    "print(targets)\n",
    "[(tokenizer._convert_id_to_token(d), ta) for d,ta in zip(data[2:-1], targets[2:-1])]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 0, -1: -1, 30: 1, 705: 2, 4: 3}\n",
      "谁能猜一猜：你大脑里神经元的总长有多少？ ”西班牙厨师被控告……“ 非常坚硬的土地。西班牙厨师被控告\n",
      "谁能猜一猜，你大脑里神经元的总长有多少？，西班牙厨师被控告， 非常坚硬的土地。西班牙厨师被控告\n",
      "[0, 19874, 6, 1580, 6, 84952, 6, 45690, 6, 84952, 4, 73675, 6, 157938, 6, 2008, 6, 133614, 6, 112535, 6, 7051, 6, 3846, 6, 138561, 705, 4, 54222, 6, 195223, 6, 17061, 6, 1317, 6, 17154, 6, 22292, 4, 4528, 6, 73613, 6, 21344, 6, 43, 6, 20770, 30, 54222, 6, 195223, 6, 17061, 6, 1317, 6, 17154, 6, 22292, 6, 2]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁', 0),\n",
       " ('能', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " ('▁', 0),\n",
       " ('▁一', 0),\n",
       " ('▁', 0),\n",
       " ('猜', 0),\n",
       " (',', 3),\n",
       " ('▁你', 0),\n",
       " ('▁', 0),\n",
       " ('大脑', 0),\n",
       " ('▁', 0),\n",
       " ('里', 0),\n",
       " ('▁', 0),\n",
       " ('神经', 0),\n",
       " ('▁', 0),\n",
       " ('元的', 0),\n",
       " ('▁', 0),\n",
       " ('总', 0),\n",
       " ('▁', 0),\n",
       " ('长', 0),\n",
       " ('▁', 0),\n",
       " ('有多少', 0),\n",
       " ('▁?', 2),\n",
       " (',', 3),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', 0),\n",
       " ('▁', 0),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', 0),\n",
       " ('▁', 0),\n",
       " ('告', 0),\n",
       " (',', 3),\n",
       " ('非常', 0),\n",
       " ('▁', 0),\n",
       " ('坚', 0),\n",
       " ('▁', 0),\n",
       " ('硬', 0),\n",
       " ('▁', 0),\n",
       " ('的', 0),\n",
       " ('▁', 0),\n",
       " ('土地', 0),\n",
       " ('。', 1),\n",
       " ('西班牙', 0),\n",
       " ('▁', 0),\n",
       " ('厨', 0),\n",
       " ('▁', 0),\n",
       " ('师', 0),\n",
       " ('▁', 0),\n",
       " ('被', 0),\n",
       " ('▁', 0),\n",
       " ('控', 0),\n",
       " ('▁', 0),\n",
       " ('告', 0),\n",
       " ('▁', 0)]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encoded_texts, targets = create_target(transcripts[164])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(datasets[0][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "encoded_texts, targets = [], []\n",
    "\n",
    "for ds in datasets:\n",
    "    trgts = []\n",
    "    for ts in ds:\n",
    "        trgts.extend([ts[i:i+512] for i in range(0,len(ts),512)])\n",
    "    x = list(zip(*(create_target(trgt) for trgt in tqdm(trgts))))\n",
    "    encoded_texts.append(x[0])\n",
    "    targets.append(x[1])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/8816 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e9a641329b542379cd53498e6a70c74"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f28fa2f7dcd40eea4e2a81024dc1aa9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e968d8e1f694b3781c62ee8b2023611"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encoded_words, targets\n",
    "comma_count = 0\n",
    "word_count = 0\n",
    "q_count = 0\n",
    "p_count = 0\n",
    "\n",
    "for tar in targets[0]:\n",
    "    for ta in tar:\n",
    "        comma_count += 1 if (ta == 3) else 0\n",
    "        word_count += 1 if (ta != -1) else 0\n",
    "        q_count += 1 if (ta == 2) else 0\n",
    "        p_count += 1 if (ta == 1) else 0\n",
    "        \n",
    "print(comma_count, word_count, q_count, p_count)\n",
    "            \n",
    "\n",
    "'''\n",
    "for te, ta in zip(encoded_texts[0][0], targets[0][0]):\n",
    "    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "188165 2339461 10215 139619\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor te, ta in zip(encoded_texts[0][0], targets[0][0]):\\n    print(f\"{tokenizer._convert_id_to_token(te):15}\\t{ta}\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "os.makedirs(data_path + model_type, exist_ok=True)\n",
    "\n",
    "for i, name in enumerate(('train', 'valid', 'test')):\n",
    "    with open(data_path + f'{model_type}/{name}_data.pkl', 'wb') as f:\n",
    "        pickle.dump((encoded_texts[i], targets[i]), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9004\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "\n",
    "for ds_targets in targets:\n",
    "    c = Counter((target for t in ds_targets for target in t))\n",
    "    print('\\t'.join([str(c[i]) for i in (1,2,3,0,-1)]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "139619\t10215\t188165\t2001462\t267423\n",
      "909\t71\t1225\t15141\t1899\n",
      "1100\t46\t1120\t16208\t2072\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e = []\n",
    "i = 0\n",
    "\n",
    "raw_words = datasets[1][2].split(' ')\n",
    "\n",
    "for te, ta in zip(encoded_texts[1][2], targets[1][2]):\n",
    "    if ta == -1:\n",
    "        e.append(te)\n",
    "    else:\n",
    "        e.append(te)\n",
    "        print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t{raw_words[i]}\")\n",
    "        e = []\n",
    "        i += 1\n",
    "print(f\"{tokenizer.decode(e):15}\\t{tokenizer.decode(target2id[ta]):10}\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# print(tokenizer.decode(encoded_texts[0][5]))\n",
    "print(targets[0][5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s>小孩 的 母亲 们,应该 买 她们 认为 健康 的东西, 但 这些 东西 实际上 有 毒。 这 导致 了  日本 一系列 的 其他 运动。 在 这一点 上,我真的 非常 骄傲 的 说,在日本 买 任何 东西 都 很难 这是 标签 贴 错了,即使 他们 仍然  在 出售 鲸 肉, 而 我认为 他们 不 该 这么 做。 但是 至少 标签 帖 对 了 你就 不 回 再 去 买 有 毒 的 海 豚 肉。并不是 只有  日本 才 这样,而是  在 一些 国家的 自然 食物 链 都 这样  在 加拿大 北部,在美国 还有 欧洲 北部,海 豹 和 鲸 鱼 的 自然 食物 链 导致 了  pc  b 分子 的 富 集  从 世界上 的 各个 地方 聚集 到 妇女 的 身上。这些 妇女 的 乳 汁 含 毒。她们 不能 用 她们 的 乳 汁 来 喂 养 她们 的 孩子们 因为 富 集 的 毒 素  在 她们 的食物 链 之中, 在 她们 世界中 的一部分  在 海洋 金 字 塔 食物 链 里。 这 说明 她们 的 免疫 系统 已经 受到 危害。 这 说明 她们 后 代 的 生长 发育 也会 受到 危害。近 十年 世界上 对 这一 问题的 关注 已经 帮助 这些 妇女 解决 了 这个问题,不是 通过 改变 食物 链 结构,而是 改变 她们 特 有的 饮食。 我们 已经 让 这些 妇女 脱离 自然 的食物 链 目的 就是 解决 这个问题。 对于 这个 特别 尖 锐 的问题,这是 个 好 办法, 但 它 对 解决 金 字 塔 食物 链 问题 没什么 帮助。还有 另一种 方法 打破 金 字 塔 食物 链。 如果 我们在 金 字 塔 食物 链 底 部 塞 入 一些 东西,</s>\n",
      "[-1, 42328, 0, 43, 0, 49889, 0, 9144, 4, 10730, 0, 11795, 0, 76026, 0, 9413, 0, 6278, 0, 39151, 4, 53072, 0, 5001, 0, 25840, 0, 59814, 0, 465, 0, 20291, 30, 63982, 0, 24754, 0, 274, 0, 34891, 0, 42652, 0, 43, 0, 5610, 0, 28188, 30, 6376, 0, 94772, 0, 575, 4, 178084, 0, 4528, 0, 196881, 0, 43, 0, 1612, 4, 133949, 0, 11795, 0, 7368, 0, 25840, 0, 1198, 0, 51059, 0, 8513, 0, 168171, 0, 48233, 0, 172138, 4, 28010, 0, 2963, 0, 29137, 0, 6376, 0, 92199, 0, 244007, 0, 8919, 4, 56584, 0, 138564, 0, 2963, 0, 562, 0, 4941, 0, 15708, 0, 2213, 30, 85264, 0, 26624, 0, 168171, 0, 150724, 0, 1036, 0, 274, 0, 69532, 0, 562, 0, 2490, 0, 2058, 0, 1677, 0, 11795, 0, 465, 0, 20291, 0, 43, 0, 2767, 0, 182157, 0, 8919, 30, 47586, 0, 7197, 0, 34891, 0, 4395, 0, 8187, 4, 17127, 0, 6376, 0, 4321, 0, 75817, 0, 7200, 0, 37717, 0, 105986, 0, 1198, 0, 8187, 0, 6376, 0, 40512, 0, 108185, 4, 75140, 0, 9679, 0, 30232, 0, 108185, 4, 2767, 0, 162921, 0, 264, 0, 244007, 0, 34460, 0, 43, 0, 7200, 0, 37717, 0, 105986, 0, 24754, 0, 274, 0, 36719, 0, 876, 0, 61511, 0, 43, 0, 16400, 0, 5525, 0, 66816, 0, 70671, 0, 43, 0, 64172, 0, 10491, 0, 128363, 0, 789, 0, 110906, 0, 43, 0, 25753, 30, 5001, 0, 110906, 0, 43, 0, 20415, 0, 64771, 0, 16005, 0, 20291, 30, 76026, 0, 5292, 0, 1173, 0, 76026, 0, 43, 0, 20415, 0, 64771, 0, 1589, 0, 131349, 0, 27670, 0, 76026, 0, 43, 0, 66717, 0, 4673, 0, 16400, 0, 5525, 0, 43, 0, 20291, 0, 10346, 0, 6376, 0, 76026, 0, 108206, 0, 105986, 0, 38152, 4, 6376, 0, 76026, 0, 146918, 0, 102988, 0, 6376, 0, 45550, 0, 2006, 0, 7234, 0, 18248, 0, 37717, 0, 105986, 0, 2008, 30, 63982, 0, 37090, 0, 76026, 0, 43, 0, 126955, 0, 8488, 0, 3990, 0, 15892, 0, 67804, 30, 63982, 0, 37090, 0, 76026, 0, 1826, 0, 5260, 0, 43, 0, 179952, 0, 187380, 0, 32218, 0, 15892, 0, 67804, 30, 4169, 0, 48403, 0, 70671, 0, 1036, 0, 11833, 0, 90873, 0, 17900, 0, 3990, 0, 12658, 0, 5001, 0, 110906, 0, 14183, 0, 274, 0, 91855, 4, 4579, 0, 4511, 0, 24471, 0, 37717, 0, 105986, 0, 30530, 4, 17127, 0, 24471, 0, 76026, 0, 2657, 0, 21747, 0, 125760, 30, 57818, 0, 3990, 0, 3933, 0, 5001, 0, 110906, 0, 222971, 0, 7200, 0, 108206, 0, 105986, 0, 16205, 0, 2252, 0, 14183, 0, 91855, 30, 69184, 0, 2935, 0, 21532, 0, 46410, 0, 138590, 0, 17085, 4, 8513, 0, 3294, 0, 1322, 0, 30328, 4, 53072, 0, 3800, 0, 1036, 0, 14183, 0, 2006, 0, 7234, 0, 18248, 0, 37717, 0, 105986, 0, 4258, 0, 125602, 0, 12658, 30, 9679, 0, 228110, 0, 5796, 0, 106444, 0, 2006, 0, 7234, 0, 18248, 0, 37717, 0, 105986, 30, 35560, 0, 82168, 0, 2006, 0, 7234, 0, 18248, 0, 37717, 0, 105986, 0, 11298, 0, 2649, 0, 23323, 0, 2283, 0, 4321, 0, 25840, 4, -1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('mPunct': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "06104f49c891daee45eafca5ae03f03e0f4b8073189a7d11a82672024b1da1ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}