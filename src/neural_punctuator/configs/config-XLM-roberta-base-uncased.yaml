debug:
  summary_writer: true
  break_train_loop: false

experiment:
  name: xlm-roberta-base

data:
  data_path: /content/drive/MyDrive/SUD_PROJECT/neural-punctuator/dataset/dual/xlm-roberta-base/
  output_labels: [empty, period, question, comma]

preprocessor:
  preproc_arg: TODO

model:
  name: ted-talks
  load_model_repo: xlm-roberta-base
  bert_output_dim: 768
  linear_hidden_dim: 1568
  num_classes: 4
  seq_len: 256
  dropout: 0.2

  save_model: true
  save_model_path: /content/drive/MyDrive/SUD_PROJECT/neural-punctuator/models-xlm-roberta

trainer:
  use_gpu: cuda:0
  batch_size: 4
  num_epochs: 12
  shuffle: true
  optimizer: radam
  loss: NLLLoss
  base_learning_rate: 0.00003 #3e-5
  classifier_learning_rate: 0.0001 #1e-4
  metrics: [precision, recall, f-score, auc]
  warmup_steps: 300
  clip_seq: 32
  grad_clip: 1.5
  weight_decay: 0.01
  train_bert: true
  load_model: # xlm-roberta-base-epoch-0.pth
  seq_shift: 0   # Shift each training sample +-seq_shift to get more different samples
  show_confusion_matrix: false

